{"examples/knowledge_graph/KnowledgeGraphDemo.py": "examples/knowledge_graph/KnowledgeGraphDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n\n# ## Using Knowledge Graph\n\n# #### Building the Knowledge Graph\n\n# In[3]:\n\n\nfrom llama_index import SimpleDirectoryReader, LLMPredictor, ServiceContext\nfrom llama_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex\nfrom langchain import OpenAI\nfrom IPython.display import Markdown, display\n\n\n# In[4]:\n\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[4]:\n\n\n# define LLM\n# NOTE: at the time of demo, text-davinci-002 did not have rate-limit errors\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n\n\n# In[ ]:\n\n\n# NOTE: can take a while! \nindex = GPTKnowledgeGraphIndex.from_documents(\n    documents, \n    max_triplets_per_chunk=2,\n    service_context=service_context\n)\n\n\n# In[24]:\n\n\nindex.save_to_disk('index_kg.json')\n\n\n# In[10]:\n\n\n# try loading\nnew_index = GPTKnowledgeGraphIndex.load_from_disk('index_kg.json', service_context=service_context)\n\n\n# #### [Optional] Try building the graph and manually add triplets!\n\n# #### Querying the Knowledge Graph\n\n# In[11]:\n\n\nresponse = new_index.query(\n    \"Tell me more about Interleaf\", \n    include_text=False, \n    response_mode=\"tree_summarize\"\n)\n\n\n# In[12]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[13]:\n\n\nresponse = new_index.query(\n    \"Tell me more about what the author worked on at Interleaf\", \n    include_text=True, \n    response_mode=\"tree_summarize\"\n)\n\n\n# In[14]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# #### Query with embeddings\n\n# In[6]:\n\n\n# NOTE: can take a while! \nindex = GPTKnowledgeGraphIndex.from_documents(\n    documents, \n    max_triplets_per_chunk=2,\n    service_context=service_context,\n    include_embeddings=True\n)\n\n\n# In[7]:\n\n\nindex.save_to_disk('index_kg_embeddings.json')\n\n\n# In[17]:\n\n\n# try loading\nnew_index = GPTKnowledgeGraphIndex.load_from_disk('index_kg_embeddings.json', service_context=service_context)\n\n\n# In[18]:\n\n\n# query using top 3 triplets plus keywords (duplicate triplets are removed)\nresponse = new_index.query(\n    \"Tell me more about what the author worked on at Interleaf\", \n    include_text=True, \n    response_mode=\"tree_summarize\",\n    embedding_mode='hybrid',\n    similarity_top_k=5\n)\n\n\n# In[19]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# #### Visualizing the Graph\n\n# In[27]:\n\n\n## create graph\nfrom pyvis.network import Network\n\ng = new_index.get_networkx_graph()\nnet = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\nnet.from_nx(g)\nnet.show(\"example.html\")\n\n\n# #### [Optional] Try building the graph and manually add triplets!\n\n# In[ ]:\n\n\nfrom llama_index.node_parser import SimpleNodeParser\n\n\n# In[ ]:\n\n\nnode_parser = SimpleNodeParser()\n\n\n# In[ ]:\n\n\nnodes = node_parser.get_nodes_from_documents(documents)\n\n\n# In[ ]:\n\n\n# initialize an empty index for now \nindex = GPTKnowledgeGraphIndex(\n    [],\n    service_context=service_context,\n)\n\n\n# In[ ]:\n\n\n# add keyword mappings and nodes manually\n# add triplets (subject, relationship, object) \n\n# for node 0\nnode_0_tups = [(\"author\", \"worked on\", \"writing\"), (\"author\", \"worked on\", \"programming\")]\nfor tup in node_0_tups:\n    index.upsert_triplet_and_node(tup, nodes[0])\n    \n# for node 1\nnode_1_tups = [\n    ('Interleaf', 'made software for', 'creating documents'),\n    ('Interleaf', 'added', 'scripting language'),\n    ('software', 'generate', 'web sites')\n]\nfor tup in node_1_tups:\n    index.upsert_triplet_and_node(tup, nodes[1])\n\n\n# In[ ]:\n\n\nresponse = index.query(\n    \"Tell me more about Interleaf\", \n    include_text=False, \n    response_mode=\"tree_summarize\"\n)\n\n\n# In[ ]:\n\n\nstr(response)\n\n", "examples/azure_demo/AzureOpenAI.py": "examples/azure_demo/AzureOpenAI.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Using Azure OpenAI resource with GPT-Index\n\n# Azure openAI resources unfortunately differ from standard openAI resources as you can't generate embeddings unless you use an embedding model. The regions where these models are available can be found here: https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models#embeddings-models\n# \n# Furthermore the regions that support embedding models unfortunately don't support the latest versions (<*>-003) of openAI models, so we are forced to use one region for embeddings and another for the text generation.\n\n# In[13]:\n\n\nimport os\nimport json\nimport openai\nfrom langchain.llms import AzureOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom llama_index import LangchainEmbedding\nfrom llama_index import (\n    GPTSimpleVectorIndex,\n    SimpleDirectoryReader, \n    LLMPredictor,\n    PromptHelper,\n    ServiceContext\n)\n\n\n# This is the API setup for the embedding model\n\n# In[10]:\n\n\nopenai.api_type = \"azure\"\nopenai.api_base = \"<insert api base url from azure>\"\nopenai.api_version = \"2022-12-01\"\nos.environ[\"OPENAI_API_KEY\"] = \"<insert api key from azure>\"\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\n# And here you can see the setup for the text generation model, you should note we use deployment name instead of model name as an arguement. This name is the one chosen when deploying the model in Azure.\n\n# In[6]:\n\n\nllm = AzureOpenAI(deployment_name=\"<insert deployment name from azure>\", model_kwargs={\n    \"api_key\": openai.api_key,\n    \"api_base\": openai.api_base,\n    \"api_type\": openai.api_type,\n    \"api_version\": openai.api_version,\n})\nllm_predictor = LLMPredictor(llm=llm)\n\nembedding_llm = LangchainEmbedding(OpenAIEmbeddings(\n    document_model_name=\"<insert deployment name from azure for embedding-doc model>\",\n    query_model_name=\"<insert deployment name from azure for embedding-query model>\"\n))\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[14]:\n\n\n# max LLM token input size\nmax_input_size = 500\n# set number of output tokens\nnum_output = 48\n# set maximum chunk overlap\nmax_chunk_overlap = 20\n\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\n\n# In[ ]:\n\n\nservice_context = ServiceContext.from_defaults(\n    llm_predictor=llm_predictor,\n    embed_model=embedding_llm,\n    prompt_helper=prompt_helper\n)\n\n\n# In[15]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n\n\n# In[17]:\n\n\nquery = 'What is most interesting about this essay?'\nanswer = index.query(query)\n\nprint(answer.get_formatted_sources())\nprint('query was:', query)\nprint('answer was:', answer)\n\n\n# In[ ]:\n\n\n\n\n", "examples/data_connectors/TwitterDemo.py": "examples/data_connectors/TwitterDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom llama_index import GPTSimpleVectorIndex, TwitterTweetReader\nfrom IPython.display import Markdown, display\nimport os\n\n\n# In[ ]:\n\n\n# create an app in https://developer.twitter.com/en/apps\nBEARER_TOKEN = \"<bearer_token>\"\n\n\n# In[ ]:\n\n\n# create reader, specify twitter handles\nreader = TwitterTweetReader(BEARER_TOKEN)\ndocuments = reader.load_data([\"@twitter_handle1\"])\n\n\n# In[ ]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/DatabaseReaderDemo.py": "examples/data_connectors/DatabaseReaderDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom __future__ import absolute_import\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"\"\n\nfrom llama_index.readers.database import DatabaseReader\nfrom llama_index import GPTSimpleVectorIndex\n\n\n# In[ ]:\n\n\n# Initialize DatabaseReader object with the following parameters:\n\ndb = DatabaseReader(\n    scheme = \"postgresql\", # Database Scheme\n    host = \"localhost\", # Database Host\n    port = \"5432\", # Database Port\n    user = \"postgres\", # Database User\n    password = \"FakeExamplePassword\", # Database Password\n    dbname = \"postgres\", # Database Name\n)\n\n\n# In[ ]:\n\n\n### DatabaseReader class ###\n# db is an instance of DatabaseReader:\nprint(type(db))\n# DatabaseReader available method:\nprint(type(db.load_data))\n\n### SQLDatabase class ###\n# db.sql is an instance of SQLDatabase:\nprint(type(db.sql_database))\n# SQLDatabase available methods:\nprint(type(db.sql_database.from_uri))\nprint(type(db.sql_database.get_single_table_info))\nprint(type(db.sql_database.get_table_columns))\nprint(type(db.sql_database.get_table_info))\nprint(type(db.sql_database.get_table_names))\nprint(type(db.sql_database.insert_into_table))\nprint(type(db.sql_database.run))\nprint(type(db.sql_database.run_sql))\n# SQLDatabase available properties:\nprint(type(db.sql_database.dialect))\nprint(type(db.sql_database.engine))\nprint(type(db.sql_database.table_info))\n\n\n# In[ ]:\n\n\n### Testing DatabaseReader\n### from SQLDatabase, SQLAlchemy engine and Database URI:\n\n# From SQLDatabase instance:\nprint(type(db.sql_database))\ndb_from_sql_database = DatabaseReader(sql_database = db.sql_database)\nprint(type(db_from_sql_database))\n\n# From SQLAlchemy engine:\nprint(type(db.sql_database.engine))\ndb_from_engine = DatabaseReader(engine = db.sql_database.engine)\nprint(type(db_from_engine))\n\n# From Database URI:\nprint(type(db.uri))\ndb_from_uri = DatabaseReader(uri = db.uri)\nprint(type(db_from_uri))\n\n\n# In[ ]:\n\n\n# The below SQL Query example returns a list values of each row\n# with concatenated text from the name and age columns\n# from the users table where the age is greater than or equal to 18\n\nquery = f\"\"\"\n    SELECT\n        CONCAT(name, ' is ', age, ' years old.') AS text\n    FROM public.users\n    WHERE age >= 18\n    \"\"\"\n\n\n# In[ ]:\n\n\n# Please refer to llama_index.langchain_helpers.sql_wrapper\n# SQLDatabase.run_sql method\ntexts = db.sql_database.run_sql(command = query)\n\n# Display type(texts) and texts\n# type(texts) must return <class 'list'>\nprint(type(texts))\n\n# Documents must return a list of Tuple objects\nprint(texts)\n\n\n# In[ ]:\n\n\n# Please refer to llama_index.readers.database.DatabaseReader.load_data\n# DatabaseReader.load_data method\ndocuments = db.load_data(query = query)\n\n# Display type(documents) and documents\n# type(documents) must return <class 'list'>\nprint(type(documents))\n\n# Documents must return a list of Document objects\nprint(documents)\n\n\n# In[ ]:\n\n\ntry:\n    # Try to load existing Index from disk\n    index = GPTSimpleVectorIndex.load_from_disk('index.json')\nexcept:\n    index = GPTSimpleVectorIndex.from_documents(documents)\n\n    # Save newly created Index to disk\n    index.save_to_disk('index.json')\n\n", "examples/data_connectors/GoogleDocsDemo.py": "examples/data_connectors/GoogleDocsDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Google Docs Demo\n# Demonstrates our Google Docs data connector\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, GoogleDocsReader\nfrom IPython.display import Markdown, display\nimport os\n\n\n# In[ ]:\n\n\n# make sure credentials.json file exists\ndocument_ids = [\"<document_id>\"]\ndocuments = GoogleDocsReader().load_data(document_ids=document_ids)\n\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/SlackDemo.py": "examples/data_connectors/SlackDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Slack Demo\n# Demonstrates our Slack data connector\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, SlackReader\nfrom IPython.display import Markdown, display\nimport os\n\n\n# In[ ]:\n\n\nslack_token = os.getenv(\"SLACK_BOT_TOKEN\")\nchannel_ids = [\"<channel_id>\"]\ndocuments = SlackReader(slack_token=slack_token).load_data(channel_ids=channel_ids)\n\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/MboxReaderDemo.py": "examples/data_connectors/MboxReaderDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\nget_ipython().run_line_magic('env', 'OPENAI_API_KEY=sk-************')\n\n\n# In[2]:\n\n\nfrom llama_index import MboxReader, GPTSimpleVectorIndex\n\n\n# In[ ]:\n\n\ndocuments = MboxReader().load_data('mbox_data_dir', max_count=1000) # Returns list of documents \n\n\n# In[ ]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents) # Initialize index with documents\n\n\n# In[4]:\n\n\nres = index.query('When did i have that call with the London office?')\n\n\n# In[ ]:\n\n\nres.response\n\n", "examples/data_connectors/NotionDemo.py": "examples/data_connectors/NotionDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Notion Demo\n# Demonstrates our Notion data connector\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[1]:\n\n\nfrom llama_index import GPTListIndex, NotionPageReader\nfrom IPython.display import Markdown, display\nimport os\n\n\n# In[ ]:\n\n\nintegration_token = os.getenv(\"NOTION_INTEGRATION_TOKEN\")\npage_ids = [\"<page_id>\"]\ndocuments = NotionPageReader(integration_token=integration_token).load_data(page_ids=page_ids)\n\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# You can also pass the id of a database to index all the pages in that database:\n\n# In[4]:\n\n\ndatabase_id = \"<database-id>\"\n\n# https://developers.notion.com/docs/working-with-databases for how to find your database id\n\ndocuments = NotionPageReader(integration_token=integration_token).load_data(database_id=database_id)\n\nprint(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nindex = GPTListIndex.from_documents(documents)\nresponse = index.query(\"<query_text>\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/GithubRepositoryReaderDemo.py": "examples/data_connectors/GithubRepositoryReaderDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\n# This is due to the fact that we use asyncio.loop_until_complete in\n# the DiscordReader. Since the Jupyter kernel itself runs on\n# an event loop, we need to add some help with nesting\nget_ipython().system('pip install nest_asyncio')\nimport nest_asyncio\nnest_asyncio.apply()\n\n\n# In[ ]:\n\n\nget_ipython().run_line_magic('env', 'OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\nfrom llama_index import GPTSimpleVectorIndex, GithubRepositoryReader\nfrom IPython.display import Markdown, display\nimport os\n\n\n# In[ ]:\n\n\nget_ipython().run_line_magic('env', 'GITHUB_TOKEN=github_pat_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\ngithub_token = os.environ.get(\"GITHUB_TOKEN\")\nowner = \"jerryjliu\"\nrepo = \"gpt_index\"\nbranch = \"main\"\n\ndocuments = GithubRepositoryReader(\n    github_token=github_token,\n    owner=owner,\n    repo=repo,\n    use_parser=False,\n    verbose=False,\n).load_data(branch=branch)\n\n\n# In[ ]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents)\nindex.save_to_disk(\"github_index.json\")\n\n\n# In[ ]:\n\n\n# import time\n# for document in documents:\n#     print(document.extra_info)\n#     time.sleep(.25) \nresponse = index.query(\"What is the difference between GPTSimpleVectorIndex and GPTListIndex?\", verbose=True)\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/DiscordDemo.py": "examples/data_connectors/DiscordDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Discord Demo\n# Demonstrates our Discord data connector\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\n# This is due to the fact that we use asyncio.loop_until_complete in\n# the DiscordReader. Since the Jupyter kernel itself runs on\n# an event loop, we need to add some help with nesting\nget_ipython().system('pip install nest_asyncio')\nimport nest_asyncio\nnest_asyncio.apply()\n\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, DiscordReader\nfrom IPython.display import Markdown, display\nimport os\n\n\n# In[ ]:\n\n\ndiscord_token = os.getenv(\"DISCORD_TOKEN\")\nchannel_ids = [1057178784895348746]  # Replace with your channel_id\ndocuments = DiscordReader(discord_token=discord_token).load_data(channel_ids=channel_ids)\n\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/WeaviateDemo.py": "examples/data_connectors/WeaviateDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Weaviate\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[3]:\n\n\nimport weaviate\nfrom llama_index.readers.weaviate import WeaviateReader\n\n\n# In[ ]:\n\n\n# See https://weaviate.io/developers/weaviate/current/client-libraries/python.html\n# for more details on authentication\nresource_owner_config = weaviate.AuthClientPassword(\n  username = \"<username>\", \n  password = \"<password>\", \n)\n\n# initialize reader\nreader = WeaviateReader(\"https://<cluster-id>.semi.network/\", auth_client_secret=resource_owner_config)\n\n\n# You have two options for the Weaviate reader: 1) directly specify the class_name and properties, or 2) input the raw graphql_query. Examples are shown below.\n\n# In[4]:\n\n\n# 1) load data using class_name and properties\n# docs = reader.load_data(\n#    class_name=\"Author\", properties=[\"name\", \"description\"], separate_documents=True\n# )\n\ndocuments = reader.load_data(\n    class_name=\"<class_name>\", \n    properties=[\"property1\", \"property2\", \"...\"], \n    separate_documents=True\n)\n\n\n# In[4]:\n\n\n# 2) example GraphQL query\n# query = \"\"\"\n# {\n#   Get {\n#     Author {\n#       name\n#       description\n#     }\n#   }\n# }\n# \"\"\"\n# docs = reader.load_data(graphql_query=query, separate_documents=True)\n\nquery = \"\"\"\n{\n  Get {\n    <class_name> {\n      <property1>\n      <property2>\n      ...\n    }\n  }\n}\n\"\"\"\n\ndocuments = reader.load_data(graphql_query=query, separate_documents=True)\n\n\n# ### Create index\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/QdrantDemo.py": "examples/data_connectors/QdrantDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Qdrant Demo\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom llama_index.readers.qdrant import QdrantReader\n\n\n# In[ ]:\n\n\nreader = QdrantReader(host=\"localhost\")\n\n\n# In[ ]:\n\n\n# the query_vector is an embedding representation of your query_vector\n# Example query vector:\n#   query_vector=[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n\nquery_vector=[n1, n2, n3, ...]\n\n\n# In[ ]:\n\n\n# NOTE: Required args are collection_name, query_vector.\n# See the Python client: https://github.com/qdrant/qdrant_client\n# for more details. \ndocuments = reader.load_data(collection_name=\"demo\", query_vector=query_vector, limit=5)\n\n\n# ### Create index\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/MongoDemo.py": "examples/data_connectors/MongoDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # MongoDB Demo\n# Demonstrates our MongoDB data connector\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, SimpleMongoReader\nfrom IPython.display import Markdown, display\nimport os\n\n\n# In[ ]:\n\n\nhost = \"<host>\"\nport = \"<port>\"\ndb_name = \"<db_name>\"\ncollection_name = \"<collection_name>\"\n# query_dict is passed into db.collection.find()\nquery_dict = {}\nreader = SimpleMongoReader(host, port)\ndocuments = reader.load_data(db_name, collection_name, query_dict=query_dict)\n\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/PineconeDemo.py": "examples/data_connectors/PineconeDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Pinecone Demo\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[1]:\n\n\napi_key = \"<api_key>\"\n\n\n# In[2]:\n\n\nfrom llama_index.readers.pinecone import PineconeReader\n\n\n# In[3]:\n\n\nreader = PineconeReader(api_key=api_key, environment=\"us-west1-gcp\")\n\n\n# In[4]:\n\n\n# the id_to_text_map specifies a mapping from the ID specified in Pinecone to your text. \nid_to_text_map = {\n    \"id1\": \"text blob 1\",\n    \"id2\": \"text blob 2\",\n}\n\n# the query_vector is an embedding representation of your query_vector\n# Example query vector:\n#   query_vector=[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n\nquery_vector=[n1, n2, n3, ...]\n\n\n# In[ ]:\n\n\n# NOTE: Required args are index_name, id_to_text_map, vector.\n# In addition, we pass-through all kwargs that can be passed into the the `Query` operation in Pinecone.\n# See the API reference: https://docs.pinecone.io/reference/query\n# and also the Python client: https://github.com/pinecone-io/pinecone-python-client\n# for more details. \ndocuments = reader.load_data(index_name='quickstart', id_to_text_map=id_to_text_map, top_k=3, vector=query_vector, separate_documents=True)\n\n\n# ### Create index \n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/ChromaDemo.py": "examples/data_connectors/ChromaDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Chroma Demo\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom gpt_index.readers.chroma import ChromaReader\n\n\n# In[ ]:\n\n\n# The chroma reader loads data from a persisted Chroma collection.\n# This requires a collection name and a persist directory.\n\nreader = ChromaReader(\n    collection_name=\"chroma_collection\",\n    persist_directory=\"examples/data_connectors/chroma_collection\"\n)\n\n\n# In[ ]:\n\n\n# the query_vector is an embedding representation of your query.\n# Example query vector:\n#   query_vector=[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n\nquery_vector=[n1, n2, n3, ...]\n\n\n# In[ ]:\n\n\n# NOTE: Required args are collection_name, query_vector.\n# See the Python client: https://github.com/qdrant/qdrant_client\n# for more details. \ndocuments = reader.load_data(collection_name=\"demo\", query_vector=query_vector, limit=5)\n\n\n# ### Create index\n\n# In[ ]:\n\n\nfrom gpt_index.indices import GPTListIndex\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/FaissDemo.py": "examples/data_connectors/FaissDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Faiss Demo\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom gpt_index.readers.faiss import FaissReader\n\n\n# In[ ]:\n\n\n# Build the Faiss index. \n# A guide for how to get started with Faiss is here: https://github.com/facebookresearch/faiss/wiki/Getting-started\n# We provide some example code below.\n\nimport faiss\n\n# # Example Code\n# d = 8\n# docs = np.array([\n#     [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n#     [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n#     [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n#     [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n#     [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n# ])\n# # id_to_text_map is used for query retrieval\n# id_to_text_map = {\n#     0: \"aaaaaaaaa bbbbbbb cccccc\",\n#     1: \"foooooo barrrrrr\",\n#     2: \"tmp tmptmp tmp\",\n#     3: \"hello world hello world\",\n#     4: \"cat dog cat dog\"\n# }\n# # build the index\n# index = faiss.IndexFlatL2(d)\n# index.add(docs)\n\nid_to_text_map = {\n    \"id1\": \"text blob 1\",\n    \"id2\": \"text blob 2\",\n}\nindex = ...\n\n\n# In[ ]:\n\n\nreader = FaissReader(index)\n\n\n# In[ ]:\n\n\n# To load data from the Faiss index, you must specify: \n# k: top nearest neighbors\n# query: a 2D embedding representation of your queries (rows are queries)\nk = 4\nquery1 = np.array([...])\nquery2 = np.array([...])\nquery=np.array([query1, query2])\n\ndocuments = reader.load_data(query=query, id_to_text_map=id_to_text_map, k=k)\n\n\n# ### Create index\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"<query_text>\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/data_connectors/MakeDemo.py": "examples/data_connectors/MakeDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Make Demo\n# \n# We show how GPT Index can fit with your Make.com workflow by sending the GPT Index response to a scenario webhook.\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[2]:\n\n\nfrom llama_index import GPTSimpleVectorIndex\nfrom llama_index.readers import MakeWrapper\n\n\n# In[5]:\n\n\n# load index from disk\nindex = GPTSimpleVectorIndex.load_from_disk('../vector_indices/index_simple.json')\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\n# query index\nquery_str = \"What did the author do growing up?\"\nresponse = index.query(query_str)\n\n\n# In[11]:\n\n\n# Send response to Make.com webhook\nwrapper = MakeWrapper()\nwrapper.pass_response_to_webhook(\n    \"<webhook_url>,\n    response,\n    query_str\n)\n\n", "examples/data_connectors/ObsidianReaderDemo.py": "examples/data_connectors/ObsidianReaderDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\nget_ipython().run_line_magic('env', 'OPENAI_API_KEY=sk-************')\n\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[2]:\n\n\nfrom llama_index import ObsidianReader, GPTSimpleVectorIndex\n\n\n# In[ ]:\n\n\ndocuments = ObsidianReader('/Users/hursh/vault').load_data() # Returns list of documents \n\n\n# In[ ]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents) # Initialize index with documents\n\n\n# In[3]:\n\n\n# index.save_to_disk('index.json')\nindex = GPTSimpleVectorIndex.load_from_disk('index.json')\n\n\n# In[4]:\n\n\n# set Logging to DEBUG for more detailed outputs\nres = index.query('What is the meaning of life?')\n\n\n# In[6]:\n\n\nres.response\n\n\n# In[ ]:\n\n\n\n\n", "examples/data_connectors/WebPageDemo.py": "examples/data_connectors/WebPageDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Web Page Demo\n# \n# Demonstrates our web page reader.\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# #### Using SimpleWebPageReader\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, SimpleWebPageReader\nfrom IPython.display import Markdown, display\nimport os\n\n\n# In[ ]:\n\n\n# NOTE: the html_to_text=True option requires html2text to be installed\n\n\n# In[ ]:\n\n\ndocuments = SimpleWebPageReader(html_to_text=True).load_data([\"http://paulgraham.com/worked.html\"])\n\n\n# In[ ]:\n\n\ndocuments[0]\n\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do growing up?\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# #### Using TrafilaturaWebReader\n\n# In[ ]:\n\n\nfrom llama_index import TrafilaturaWebReader\n\n\n# In[ ]:\n\n\ndocuments = TrafilaturaWebReader().load_data([\"http://paulgraham.com/worked.html\"])\n\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do growing up?\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# ### Using RssReader\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, RssReader\n\ndocuments = RssReader().load_data([\n    \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"\n    ])\n\nindex = GPTListIndex.from_documents(documents)\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What happened in the news today?\")\n\n", "examples/playground/PlaygroundDemo.py": "examples/playground/PlaygroundDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# In[2]:\n\n\n# Hide INFO logs regarding token usage, etc\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.CRITICAL)\n\n\n# ## Setup\n# \n# ### Generate some example Documents\n\n# In[ ]:\n\n\nfrom llama_index import download_loader\nfrom llama_index.indices.vector_store import GPTSimpleVectorIndex\nfrom llama_index.indices.tree.base import GPTTreeIndex\n\nWikipediaReader = download_loader(\"WikipediaReader\")\n\nloader = WikipediaReader()\ndocuments = loader.load_data(pages=['Berlin'])\n\n\n# ### Create a list of any sort of indices (custom LLMs, custom embeddings, etc)\n\n# In[ ]:\n\n\nindices = [GPTSimpleVectorIndex.from_documents(documents), GPTTreeIndex.from_documents(documents)]\n\n\n# ## Using the Playground\n# \n# \n# ### Initialize with indices\n\n# In[4]:\n\n\nfrom llama_index.playground import Playground\n\nplayground = Playground(indices=indices)\n\n\n# In[5]:\n\n\nplayground.compare(\"What is the population of Berlin?\")\n\n\n# ### Initialize with Documents\n\n# In[ ]:\n\n\n# Uses documents in a preset list of indices\nplayground = Playground.from_docs(documents=documents)\n\n", "examples/optimizer/OptimizerDemo.py": "examples/optimizer/OptimizerDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# ### Setup\n\n# In[9]:\n\n\nfrom llama_index import download_loader\n\nWikipediaReader = download_loader(\"WikipediaReader\")\n\nloader = WikipediaReader()\ndocuments = loader.load_data(pages=['Berlin'])\n\n\n# In[10]:\n\n\nfrom llama_index import GPTSimpleVectorIndex\nindex = GPTSimpleVectorIndex.from_documents(documents)\n# save index to file\nindex.save_to_disk(\"simple_vector_index.json\")\n\n\n# Compare query with and without optimization for LLM token usage, Embedding Model usage on query, Embedding model usage for optimizer, and total time.\n\n# In[13]:\n\n\nimport time\nfrom gpt_index import GPTSimpleVectorIndex\nfrom gpt_index.optimization.optimizer import SentenceEmbeddingOptimizer\n# load from disk\nindex = GPTSimpleVectorIndex.load_from_disk('simple_vector_index.json')\n\nprint(\"Without optimization\")\nstart_time = time.time()\nres = index.query(\"What is the population of Berlin?\")\nend_time = time.time()\nprint(\"Total time elapsed: {}\".format(end_time - start_time))\nprint(\"Answer: {}\".format(res))\n\nprint(\"With optimization\")\nstart_time = time.time()\nres = index.query(\"What is the population of Berlin?\", optimizer=SentenceEmbeddingOptimizer(percentile_cutoff=0.5))\nend_time = time.time()\nprint(\"Total time elapsed: {}\".format(end_time - start_time))\nprint(\"Answer: {}\".format(res))\n\nprint(\"Alternate optimization cutoff\")\nstart_time = time.time()\nres = index.query(\"What is the population of Berlin?\", optimizer=SentenceEmbeddingOptimizer(threshold_cutoff=0.7))\nend_time = time.time()\nprint(\"Total time elapsed: {}\".format(end_time - start_time))\nprint(\"Answer: {}\".format(res))\n\n", "examples/chatbot/Chatbot_SEC.py": "examples/chatbot/Chatbot_SEC.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[9]:\n\n\n# download files\nget_ipython().system('mkdir data')\nget_ipython().system('wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip')\nget_ipython().system('unzip data/UBER.zip -d data')\n\n\n# In[10]:\n\n\n# set text wrapping\nfrom IPython.display import HTML, display\n\ndef set_css():\n  display(HTML('''\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  '''))\nget_ipython().events.register('pre_run_cell', set_css)\n\n\n# In[11]:\n\n\nimport os\nos.environ['OPENAI_API_KEY'] = \"\"\n\n\n# In[1]:\n\n\nfrom gpt_index import download_loader, GPTSimpleVectorIndex, ServiceContext\nfrom pathlib import Path\n\n\n# ### Ingest Unstructured Data Through the Unstructured.io Reader\n# \n# Leverage the capabilities of Unstructured.io HTML parsing.\n# Downloaded through LlamaHub.\n\n# In[2]:\n\n\nyears = [2022, 2021, 2020, 2019]\n\n\n# In[3]:\n\n\nUnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n\n\n# In[3]:\n\n\nloader = UnstructuredReader()\ndoc_set = {}\nall_docs = []\nfor year in years:\n    year_docs = loader.load_data(file=Path(f'./data/UBER/UBER_{year}.html'), split_documents=False)\n    # insert year metadata into each year\n    for d in year_docs:\n        d.extra_info = {\"year\": year}\n    doc_set[year] = year_docs\n    all_docs.extend(year_docs)\n\n\n# ### Setup a Vector Index for each SEC filing\n# \n# We setup a separate vector index for each SEC filing from 2019-2022.\n# \n# We also optionally initialize a \"global\" index by dumping all files into the vector store.\n\n# In[ ]:\n\n\n# initialize simple vector indices + global vector index\n# NOTE: don't run this cell if the indices are already loaded! \nindex_set = {}\nservice_context = ServiceContext.from_defaults(chunk_size_limit=512)\nfor year in years:\n    cur_index = GPTSimpleVectorIndex.from_documents(doc_set[year], service_context=service_context)\n    index_set[year] = cur_index\n    cur_index.save_to_disk(f'index_{year}.json')\n    \n\n\n# In[6]:\n\n\n# Load indices from disk\nindex_set = {}\nfor year in years:\n    cur_index = GPTSimpleVectorIndex.load_from_disk(f'index_{year}.json')\n    index_set[year] = cur_index\n\n\n# ### Composing a Graph to synthesize answers across 10-K filings (2019-2022)\n# \n# We want our queries to aggregate/synthesize information across *all* 10-K filings. To do this, we define a List index\n# on top of the 4 vector indices.\n\n# In[7]:\n\n\nfrom gpt_index import GPTListIndex, LLMPredictor\nfrom langchain import OpenAI\nfrom gpt_index.indices.composability import ComposableGraph\n\n\n# In[8]:\n\n\n# set summary text for each doc\nindex_summaries = [f\"UBER 10-k Filing for {year} fiscal year\" for year in years]\n\n\n# In[9]:\n\n\n# set number of output tokens\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, max_tokens=512))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n\n# In[10]:\n\n\n# define a list index over the vector indices\n# allows us to synthesize information across each index\ngraph = ComposableGraph.from_indices(\n    GPTListIndex, \n    [index_set[y] for y in years], \n    index_summaries=index_summaries,\n    service_context=service_context\n)\n\n\n# In[9]:\n\n\ngraph.save_to_disk('10k_graph.json')\n\n\n# In[12]:\n\n\ngraph = ComposableGraph.load_from_disk('10k_graph.json', service_context=service_context)\n\n\n# ## Setting up the Chatbot Agent\n# \n# We use Langchain to define the outer chatbot abstraction. We use LlamaIndex as a core Tool within this abstraction.\n\n# In[13]:\n\n\nfrom langchain.agents import Tool\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import initialize_agent\n\nfrom gpt_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig, GraphToolConfig\n\n\n# In[28]:\n\n\n# define a decompose transform\nfrom gpt_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor, verbose=True\n)\n\n# define query configs for graph \nquery_configs = [\n    {\n        \"index_struct_type\": \"simple_dict\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"similarity_top_k\": 1,\n            # \"include_summary\": True\n        },\n        \"query_transform\": decompose_transform\n    },\n    {\n        \"index_struct_type\": \"list\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"response_mode\": \"tree_summarize\",\n            \"verbose\": True\n        }\n    },\n]\n\n\n# In[29]:\n\n\n# define toolkit\nindex_configs = []\nfor y in range(2019, 2023):\n    tool_config = IndexToolConfig(\n        index=index_set[y], \n        name=f\"Vector Index {y}\",\n        description=f\"useful for when you want to answer queries about the {y} SEC 10-K for Uber\",\n        index_query_kwargs={\"similarity_top_k\": 3},\n        tool_kwargs={\"return_direct\": True}\n    )\n    index_configs.append(tool_config)\n    \n# graph config\ngraph_config = GraphToolConfig(\n    graph=graph,\n    name=f\"Graph Index\",\n    description=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber.\",\n    query_configs=query_configs,\n    tool_kwargs={\"return_direct\": True}\n)\n\ntoolkit = LlamaToolkit(\n    index_configs=index_configs,\n    graph_configs=[graph_config]\n)\n\n\n# In[30]:\n\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm=OpenAI(temperature=0)\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    verbose=True\n)\n\n\n# In[15]:\n\n\nagent_chain.run(input=\"hi, i am bob\")\n\n\n# In[16]:\n\n\nagent_chain.run(input=\"What were some of the biggest risk factors in 2020 for Uber?\")\n\n\n# In[31]:\n\n\ncross_query_str = (\n    \"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"\n)\n\nagent_chain.run(input=cross_query_str)\n\n\n# ### Setup Chatbot Loop Within Notebook\n# \n# We'll keep a running loop so that you can converse with the agent. \n\n# In[39]:\n\n\n# reinitialize agent\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm=OpenAI(temperature=0)\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n)\n\n\n# In[ ]:\n\n\nwhile True:\n    text_input = input(\"User: \")\n    response = agent_chain.run(input=text_input)\n    print(f'Agent: {response}')\n    \n\n\n# In[ ]:\n\n\n\n\n", "examples/langchain_demo/LangchainDemo.py": "examples/langchain_demo/LangchainDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# ## GPT Index <> Langchain Integrations\n# \n# This demo notebook shows how you can provide integrations between GPT Index and Langchain. It provides the following examples:\n# - Using GPT Index as a callable tool with a Langchain agent\n# - Using GPT Index as a memory module; this allows you to insert arbitrary amounts of conversation history with a Langchain chatbot!\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# #### Using GPT Index as a Callable Tool\n\n# In[2]:\n\n\nfrom langchain.agents import Tool\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\nfrom langchain import OpenAI\nfrom langchain.agents import initialize_agent\n\nfrom llama_index import GPTSimpleVectorIndex\n\n\n# In[3]:\n\n\nindex = GPTSimpleVectorIndex.load_from_disk('../vector_indices/index_simple.json')\n\n\n# In[4]:\n\n\ntools = [\n    Tool(\n        name = \"GPT Index\",\n        func=lambda q: str(index.query(q)),\n        description=\"useful for when you want to answer questions about the author. The input to this tool should be a complete english sentence.\",\n        return_direct=True\n    ),\n]\n\n\n# In[5]:\n\n\n# set Logging to DEBUG for more detailed outputs\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm=OpenAI(temperature=0)\nagent_chain = initialize_agent(tools, llm, agent=\"conversational-react-description\", memory=memory)\n\n\n# In[6]:\n\n\nagent_chain.run(input=\"hi, i am bob\")\n\n\n# In[7]:\n\n\nagent_chain.run(input=\"What did the author do growing up?\")\n\n\n# #### Using GPT Index as a memory module\n\n# In[12]:\n\n\n# try using GPT List Index!\nfrom langchain import OpenAI\nfrom langchain.llms import OpenAIChat\nfrom langchain.agents import initialize_agent\n\nfrom llama_index import GPTListIndex\nfrom llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n\n\n# In[13]:\n\n\nindex = GPTListIndex([])\n\n\n# In[14]:\n\n\n# set Logging to DEBUG for more detailed outputs\n# NOTE: you can also use a conversational chain\n\nmemory = GPTIndexChatMemory(\n    index=index, \n    memory_key=\"chat_history\", \n    query_kwargs={\"response_mode\": \"compact\"},\n    # return_source returns source nodes instead of querying index\n    return_source=True,\n    # return_messages returns context in message format\n    return_messages=True\n)\nllm = OpenAIChat(temperature=0)\n# llm=OpenAI(temperature=0)\nagent_chain = initialize_agent([], llm, agent=\"conversational-react-description\", memory=memory)\n\n\n# In[15]:\n\n\nagent_chain.run(input=\"hi, i am bob\")\n\n\n# In[16]:\n\n\n# NOTE: the query now calls the GPTListIndex memory module. \nagent_chain.run(input=\"what's my name?\")\n\n\n# In[ ]:\n\n\n\n\n", "examples/struct_indices/SQLIndexDemo-ManyTables.py": "examples/struct_indices/SQLIndexDemo-ManyTables.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # SQL Index Demo\n# \n# Demo where table contains context.\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[2]:\n\n\nfrom gpt_index import GPTSQLStructStoreIndex, SQLDatabase, SimpleDirectoryReader, WikipediaReader, Document\nfrom gpt_index.indices.struct_store import SQLContextContainerBuilder\nfrom IPython.display import Markdown, display\n\n\n# ### Create Database Schema + Test Data\n# \n# Here we introduce a toy scenario where there are 100 tables (too big to fit into the prompt)\n\n# In[3]:\n\n\nfrom sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n\n\n# In[30]:\n\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData(bind=engine)\n\n\n# In[31]:\n\n\n# create city SQL table\ntable_name = \"city_stats\"\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(\"city_name\", String(16), primary_key=True),\n    Column(\"population\", Integer),\n    Column(\"country\", String(16), nullable=False),\n)\nall_table_names = [\"city_stats\"]\n# create a ton of dummy tables\nn = 100\nfor i in range(n):\n    tmp_table_name = f\"tmp_table_{i}\"\n    tmp_table = Table(\n        tmp_table_name,\n        metadata_obj,\n        Column(f\"tmp_field_{i}_1\", String(16), primary_key=True),\n        Column(f\"tmp_field_{i}_2\", Integer),\n        Column(f\"tmp_field_{i}_3\", String(16), nullable=False),\n    )\n    all_table_names.append(f\"tmp_table_{i}\")\n\nmetadata_obj.create_all()\n\n\n# In[ ]:\n\n\n# print tables\nmetadata_obj.tables.keys()\n\n\n# We introduce some test data into the `city_stats` table\n\n# In[33]:\n\n\nfrom sqlalchemy import insert\nrows = [\n    {\"city_name\": \"Toronto\", \"population\": 2930000, \"country\": \"Canada\"},\n    {\"city_name\": \"Tokyo\", \"population\": 13960000, \"country\": \"Japan\"},\n    {\"city_name\": \"Chicago\", \"population\": 2679000, \"country\": \"United States\"},\n    {\"city_name\": \"Seoul\", \"population\": 9776000, \"country\": \"South Korea\"},\n]\nfor row in rows:\n    stmt = insert(city_stats_table).values(**row)\n    with engine.connect() as connection:\n        cursor = connection.execute(stmt)\n\n\n# In[34]:\n\n\nwith engine.connect() as connection:\n    cursor = connection.exec_driver_sql(\"SELECT * FROM city_stats\")\n    print(cursor.fetchall())\n\n\n# ### Using GPT Index to Store Table Schema Context\n\n# In[35]:\n\n\nfrom gpt_index import GPTSQLStructStoreIndex, SQLDatabase, GPTSimpleVectorIndex\nfrom gpt_index.indices.struct_store import SQLContextContainerBuilder\n\n\n# In[36]:\n\n\nsql_database = SQLDatabase(engine)\n\n\n# In[ ]:\n\n\nsql_database.table_info\n\n\n# We dump the table schema information into a vector index. The vector index is stored within the context builder for future use.\n\n# In[ ]:\n\n\n# build a vector index from the table schema information\ncontext_builder = SQLContextContainerBuilder(sql_database)\ntable_schema_index = context_builder.derive_index_from_context(\n    GPTSimpleVectorIndex,\n)\n\n\n# In[ ]:\n\n\n# NOTE: not ingesting any unstructured documents atm\nindex = GPTSQLStructStoreIndex.from_documents(\n    [],\n    sql_database=sql_database, \n    table_name=\"city_stats\",\n)\n\n\n# ### Query Index\n\n# Here we show a natural language query. \n# 1. We first query for the right table schema. Note that we build a context container during query-time.\n# 2. Given this context container, we execute the NL query against the db.\n\n# In[44]:\n\n\nquery_str = \"Which city has the highest population?\"\ncontext_builder.query_index_for_context(table_schema_index, query_str, store_context_str=True)\ncontext_container = context_builder.build_context_container()\n\n\n# In[45]:\n\n\ndisplay(Markdown(f\"<b>{context_container.context_str}</b>\"))\n\n\n# In[40]:\n\n\nresponse = index.query(query_str, sql_context_container=context_container)\n\n\n# We can also use codewords during the NL query! \n\n# In[41]:\n\n\nstr(response)\n\n\n# In[ ]:\n\n\nresponse.extra_info\n\n", "examples/struct_indices/SQLIndexDemo.py": "examples/struct_indices/SQLIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # SQL Index Demo\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[19]:\n\n\nfrom llama_index import SimpleDirectoryReader, WikipediaReader\nfrom IPython.display import Markdown, display\n\n\n# ### Load Wikipedia Data\n\n# In[ ]:\n\n\n# install wikipedia python package\nget_ipython().system('pip install wikipedia')\n\n\n# In[2]:\n\n\nwiki_docs = WikipediaReader().load_data(pages=['Toronto', 'Berlin', 'Tokyo'])\n\n\n# ### Create Database Schema\n\n# In[3]:\n\n\nfrom sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n\n\n# In[4]:\n\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData(bind=engine)\n\n\n# In[5]:\n\n\n# create city SQL table\ntable_name = \"city_stats\"\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(\"city_name\", String(16), primary_key=True),\n    Column(\"population\", Integer),\n    Column(\"country\", String(16), nullable=False),\n)\nmetadata_obj.create_all()\n\n\n# ### Build Index\n\n# In[20]:\n\n\nfrom llama_index import GPTSQLStructStoreIndex, SQLDatabase, ServiceContext\nfrom langchain import OpenAI\nfrom llama_index import LLMPredictor\n\n\n# In[21]:\n\n\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n\n# In[9]:\n\n\nsql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n\n\n# In[10]:\n\n\nsql_database.table_info\n\n\n# In[ ]:\n\n\n# NOTE: the table_name specified here is the table that you\n# want to extract into from unstructured documents.\nindex = GPTSQLStructStoreIndex.from_documents(\n    wiki_docs, \n    sql_database=sql_database, \n    table_name=\"city_stats\",\n    service_context=service_context\n)\n\n\n# In[12]:\n\n\n# view current table\nstmt = select(\n    [column(\"city_name\"), column(\"population\"), column(\"country\")]\n).select_from(city_stats_table)\n\nwith engine.connect() as connection:\n    results = connection.execute(stmt).fetchall()\n    print(results)\n\n\n# ### Query Index\n\n# We first show a raw SQL query, which directly executes over the table\n\n# In[15]:\n\n\nresponse = index.query(\"SELECT city_name from city_stats\", mode=\"sql\")\n\n\n# In[14]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# Here we show a natural language query, which is translated to a SQL query under the hood\n\n# In[16]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"Which city has the highest population?\", mode=\"default\")\n\n\n# In[23]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[18]:\n\n\n# you can also fetch the raw result from SQLAlchemy! \nresponse.extra_info[\"result\"]\n\n\n# ### Using Langchain for Querying\n# \n# Since our SQLDatabase inherits from langchain, you can also use langchain itself for querying purposes.\n\n# In[22]:\n\n\nfrom langchain import OpenAI, SQLDatabase, SQLDatabaseChain\n\n\n# In[24]:\n\n\nllm = OpenAI(temperature=0)\n\n\n# In[26]:\n\n\n# set Logging to DEBUG for more detailed outputs\ndb_chain = SQLDatabaseChain(llm=llm, database=sql_database)\n\n\n# In[27]:\n\n\ndb_chain.run(\"Which city has the highest population?\")\n\n\n# In[ ]:\n\n\n\n\n", "examples/struct_indices/SQLIndexDemo-Context.py": "examples/struct_indices/SQLIndexDemo-Context.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # SQL Index Demo\n# \n# Demo where table contains context.\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom llama_index import GPTSQLStructStoreIndex, SQLDatabase, SimpleDirectoryReader, WikipediaReader, Document\nfrom llama_index.indices.struct_store import SQLContextContainerBuilder\nfrom IPython.display import Markdown, display\n\n\n# ### Load Wikipedia Data\n\n# In[ ]:\n\n\n# install wikipedia python package\nget_ipython().system('pip install wikipedia')\n\n\n# In[ ]:\n\n\nwiki_docs = WikipediaReader().load_data(pages=['Toronto', 'Berlin', 'Tokyo'])\n\n\n# ### Create Database Schema\n\n# In[ ]:\n\n\nfrom sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n\n\n# In[ ]:\n\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData(bind=engine)\n\n\n# In[ ]:\n\n\n# create city SQL table\ntable_name = \"city_stats\"\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(\"city_name\", String(16), primary_key=True),\n    Column(\"population\", Integer),\n    Column(\"country\", String(16), nullable=False),\n)\nmetadata_obj.create_all()\n\n\n# ### Build Index with Context\n\n# In[ ]:\n\n\nfrom llama_index import GPTSQLStructStoreIndex, SQLDatabase\nfrom llama_index.indices.struct_store import SQLContextContainerBuilder\n\n\n# In[ ]:\n\n\nsql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n\n\n# In[ ]:\n\n\nsql_database.table_info\n\n\n# We either set the context manually, or have GPT extract the context for us\n\n# In[ ]:\n\n\n# manually set context text\ncity_stats_text = (\n    \"This table gives information regarding the population and country of a given city.\\n\"\n    \"The user will query with codewords, where 'foo' corresponds to population and 'bar'\"\n    \"corresponds to city.\"\n)\ntable_context_dict={\"city_stats\": city_stats_text}\ncontext_builder = SQLContextContainerBuilder(sql_database, context_dict=table_context_dict)\ncontext_container = context_builder.build_context_container()\n\n\n# In[ ]:\n\n\nindex = GPTSQLStructStoreIndex.from_documents(\n    wiki_docs, \n    sql_database=sql_database, \n    table_name=\"city_stats\",\n    sql_context_container=context_container\n)\n\n\n# In[ ]:\n\n\n# extract context from a raw Document using GPT\ncity_stats_text = (\n    \"This table gives information regarding the population and country of a given city.\\n\"\n)\ncontext_documents_dict = {\"city_stats\": [Document(city_stats_text)]}\ncontext_builder = SQLContextContainerBuilder.from_documents(\n    context_documents_dict, \n    sql_database\n)\ncontext_container = context_builder.build_context_container()\n\nindex = GPTSQLStructStoreIndex.from_documents(\n    wiki_docs, \n    sql_database=sql_database, \n    table_name=\"city_stats\",\n    sql_context_container=context_container,\n)\n\n\n# In[ ]:\n\n\n# view current table\nstmt = select(\n    [column(\"city_name\"), column(\"population\"), column(\"country\")]\n).select_from(city_stats_table)\n\nwith engine.connect() as connection:\n    results = connection.execute(stmt).fetchall()\n    print(results)\n\n\n# ### Query Index\n\n# Here we show a natural language query, which is translated to a SQL query under the hood.\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"Which city has the highest population?\", mode=\"default\")\n\n\n# We can also use codewords during the NL query! \n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"Which bar has the highest foo?\", mode=\"default\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/struct_indices/PandasIndexDemo.py": "examples/struct_indices/PandasIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Pandas Index Demo\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[2]:\n\n\nfrom gpt_index import SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[3]:\n\n\nfrom gpt_index.indices.struct_store import GPTPandasIndex\nimport pandas as pd\n\n\n# ### Let's start on a Toy DataFrame\n# \n# Very simple dataframe containing city and population pairs.\n\n# In[4]:\n\n\n# Test on some sample data \ndf = pd.DataFrame(\n    {\n        \"city\": [\"Toronto\", \"Tokyo\", \"Berlin\"], \n        \"population\": [2930000, 13960000, 3645000]\n    }\n)\n\n\n# In[5]:\n\n\nindex = GPTPandasIndex(df=df)\n\n\n# In[6]:\n\n\nresponse = index.query(\n    \"What is the city with the highest population?\",\n    verbose=True\n)\n\n\n# In[7]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[8]:\n\n\n# get pandas python instructions\nprint(response.extra_info[\"pandas_instruction_str\"])\n\n\n# ### Analyzing the Titanic Dataset\n# \n# The Titanic dataset is one of the most popular tabular datasets in introductory machine learning\n# Source: https://www.kaggle.com/c/titanic\n\n# In[9]:\n\n\ndf = pd.read_csv(\"titanic_train.csv\")\n\n\n# In[10]:\n\n\nindex = GPTPandasIndex(df=df)\n\n\n# In[11]:\n\n\nresponse = index.query(\n    \"What is the correlation between survival and age?\",\n    verbose=True\n)\n\n\n# In[12]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[13]:\n\n\n# get pandas python instructions\nprint(response.extra_info[\"pandas_instruction_str\"])\n\n\n# In[ ]:\n\n\n\n\n", "examples/async/AsyncComposableIndicesSEC.py": "examples/async/AsyncComposableIndicesSEC.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[2]:\n\n\n# download files\nget_ipython().system('mkdir data')\nget_ipython().system('wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip')\nget_ipython().system('unzip data/UBER.zip -d data')\n\n\n# In[3]:\n\n\nimport nest_asyncio\nnest_asyncio.apply()\n\n\n# In[4]:\n\n\nfrom gpt_index import download_loader, GPTSimpleVectorIndex\nfrom pathlib import Path\n\n\n# ### Ingest Unstructured Data Through the Unstructured.io Reader\n# \n# Leverage the capabilities of Unstructured.io HTML parsing.\n# Downloaded through LlamaHub.\n\n# In[5]:\n\n\nyears = [2022, 2021, 2020, 2019]\n\n\n# In[6]:\n\n\nUnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True, use_gpt_index_import=True)\n\n\n# In[7]:\n\n\nloader = UnstructuredReader()\ndoc_set = {}\nall_docs = []\nfor year in years:\n    year_docs = loader.load_data(file=Path(f'./data/UBER/UBER_{year}.html'), split_documents=False)\n    # insert year metadata into each year\n    for d in year_docs:\n        d.extra_info = {\"year\": year}\n    doc_set[year] = year_docs\n    all_docs.extend(year_docs)\n\n\n# ### Setup a Vector Index for each SEC filing\n# \n# We setup a separate vector index for each SEC filing from 2019-2022.\n# \n# We also optionally initialize a \"global\" index by dumping all files into the vector store.\n\n# In[8]:\n\n\n# initialize simple vector indices + global vector index\n# NOTE: don't run this cell if the indices are already loaded! \nindex_set = {}\nfor year in years:\n    cur_index = GPTSimpleVectorIndex(doc_set[year], chunk_size_limit=512)\n    index_set[year] = cur_index\n    cur_index.save_to_disk(f'index_{year}.json')\n    \n\n\n# In[9]:\n\n\n# Load indices from disk\nindex_set = {}\nfor year in years:\n    cur_index = GPTSimpleVectorIndex.load_from_disk(f'index_{year}.json')\n    index_set[year] = cur_index\n\n\n# ### Composing a Graph to synthesize answers across 10-K filings (2019-2022)\n# \n# We want our queries to aggregate/synthesize information across *all* 10-K filings. To do this, we define a List index\n# on top of the 4 vector indices.\n\n# In[10]:\n\n\nfrom gpt_index import GPTListIndex, LLMPredictor\nfrom langchain import OpenAI\nfrom gpt_index.composability import ComposableGraph\n\n\n# In[11]:\n\n\n# set summary text for each doc\nfor year in years:\n    index_set[year].set_text(f\"UBER 10-k Filing for {year} fiscal year\")\n\n\n# In[12]:\n\n\n# set number of output tokens\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, max_tokens=512))\n\n\n# In[13]:\n\n\n# define a list index over the vector indices\n# allows us to synthesize information across each index\nlist_index = GPTListIndex([index_set[y] for y in years], llm_predictor=llm_predictor)\n\n\n# In[14]:\n\n\ngraph = ComposableGraph.build_from_index(list_index)\n\n\n# In[15]:\n\n\ngraph.save_to_disk('10k_graph.json')\n\n\n# In[16]:\n\n\ngraph = ComposableGraph.load_from_disk('10k_graph.json', llm_predictor=llm_predictor)\n\n\n# In[17]:\n\n\n# TMP: define prompt helper\nfrom gpt_index import PromptHelper\nquery_prompt_helper = PromptHelper(4096, 256, 0)\n\n# define query configs for graph \nquery_configs = [\n    {\n        \"index_struct_type\": \"simple_dict\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"similarity_top_k\": 1,\n            # \"include_summary\": True,\n            \"response_mode\": \"tree_summarize\",\n            \"prompt_helper\": query_prompt_helper\n        },\n    },\n    {\n        \"index_struct_type\": \"list\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"response_mode\": \"tree_summarize\",\n            \"verbose\": True,\n            \"prompt_helper\": query_prompt_helper\n        }\n    },\n]\n\n\n# In[ ]:\n\n\n# TODO: add aquery to graph\nimport asyncio\nimport time\n\ncross_query_str = (\n    \"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"\n)\n\nstart_time = time.perf_counter()\ntask = graph.aquery(cross_query_str, query_configs=query_configs)\nresponse = asyncio.run(task)\nelapsed_time = time.perf_counter() - start_time\n\n\n# In[19]:\n\n\nprint(str(response))\nprint(str(elapsed_time))\n\n", "examples/async/AsyncGPTTreeIndexDemo.py": "examples/async/AsyncGPTTreeIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Async GPTTreeIndex Demo\n\n# In[ ]:\n\n\n# NOTE: This is ONLY necessary in jupyter notebook.\n# Details: Jupyter runs an event-loop behind the scenes. \n#          This results in nested event-loops when we start an event-loop to make async queries.\n#          This is normally not allowed, we use nest_asyncio to allow it for convenience.  \nimport nest_asyncio\nnest_asyncio.apply()\n\n\n# In[ ]:\n\n\nimport time\nfrom llama_index import GPTTreeIndex, SimpleDirectoryReader\n\n\n# In[ ]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# #### By default, GPTTreeIndex makes blocking LLM calls\n\n# In[ ]:\n\n\nstart_time = time.perf_counter()\nindex = GPTTreeIndex.from_documents(documents)\nelapsed_time = time.perf_counter() - start_time\n\n\n# It takes ~47s to finish building GPTTreeIndex from 5 text chunks.\n\n# #### Pass in `use_async=True` to enable asynchronous LLM calls\n\n# In[ ]:\n\n\nstart_time = time.perf_counter()\nindex = GPTTreeIndex.from_documents(documents, use_async=True)\nelapsed_time = time.perf_counter() - start_time\n\n\n# It takes ~12s to finish building the GPTTreeIndex from 5 text chunks.\n\n# In[ ]:\n\n\n\n\n", "examples/async/AsyncQueryDemo.py": "examples/async/AsyncQueryDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Async TreeSummarizeQuery Demo\n\n# In[1]:\n\n\n# NOTE: This is ONLY necessary in jupyter notebook.\n# Details: Jupyter runs an event-loop behind the scenes. \n#          This results in nested event-loops when we start an event-loop to make async queries.\n#          This is normally not allowed, we use nest_asyncio to allow it for convenience.  \nimport nest_asyncio\nnest_asyncio.apply()\n\n\n# In[2]:\n\n\nimport time\nfrom gpt_index import GPTListIndex, SimpleDirectoryReader\n\n\n# In[3]:\n\n\nquery_str = \"What is Paul Graham's biggest achievement?\"\n\n\n# In[4]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[ ]:\n\n\nindex = GPTListIndex.from_documents(documents)\n\n\n# #### By default, generate a response through hierarchical tree summarization (i.e., `response_mode=tree_summarize`) makes blocking LLM calls\n\n# In[ ]:\n\n\nstart_time = time.perf_counter()\nindex.query(query_str, response_mode='tree_summarize')\nelapsed_time = time.perf_counter() - start_time\n\n\n# It takes ~21s to generate a response through hierarchical tree summarization (i.e., `response_mode=tree_summarize`).\n\n# #### Option 1: Running `aquery` (async query call) will take advantage of async LLM calls\n\n# In[ ]:\n\n\nimport asyncio\nstart_time = time.perf_counter()\ntask = index.aquery(query_str, response_mode='tree_summarize')\nasyncio.run(task)\nelapsed_time = time.perf_counter() - start_time\n\n\n# It takes ~6.8s to generate a response through `aquery` (i.e., `response_mode=tree_summarize`).\n\n# #### Option 2: Pass in `use_async=True` to enable asynchronous LLM calls within a synchronous `query`\n# \n# This approach makes a synchronous `query` calls, but runs async tasks during the \"tree_summarize\" operation.\n\n# In[ ]:\n\n\nstart_time = time.perf_counter()\nindex.query(query_str, response_mode='tree_summarize', use_async=True)\nelapsed_time = time.perf_counter() - start_time\n\n\n# It takes ~6.9s to generate a response through hierarchical tree summarization (i.e., `response_mode=tree_summarize`).\n", "examples/async/AsyncLLMPredictorDemo.py": "examples/async/AsyncLLMPredictorDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Async LLMPredictor Demo\n\n# In[ ]:\n\n\nfrom llama_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom llama_index.prompts.default_prompts import DEFAULT_SUMMARY_PROMPT\nimport asyncio\nimport time\n\n\n# In[ ]:\n\n\ncontext_strs = [\n    'Paul Graham',\n    'Steve Jobs',\n    'Barack Obama',\n]\n\nprompt = DEFAULT_SUMMARY_PROMPT\n\n\n# In[ ]:\n\n\nllm = LLMPredictor()\n\n\n# #### By default, LLM calls are blocking (i.e. only one API request at a time)\n\n# In[ ]:\n\n\nstart_time = time.perf_counter()\noutputs = [llm.predict(prompt, context_str=context_str) for context_str in context_strs]\nelapsed_time = time.perf_counter() - start_time\n\n\n# It takes ~19s to finish all 3 calls.\n\n# #### We can enable asynchronous calls with the `apredict` function\n\n# In[ ]:\n\n\nstart_time = time.perf_counter()\ntasks = [llm.apredict(prompt, context_str=context_str) for context_str in context_strs]\nawait asyncio.gather(*tasks)\nelapsed_time = time.perf_counter() - start_time\n\n\n# It takes ~7s to finish all 3 calls.\n", "examples/cost_analysis/TokenPredictor.py": "examples/cost_analysis/TokenPredictor.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Using Token Predictors\n# \n# Using our token predictors, we can predict the token usage of an operation before actually performing it.\n# \n# We first show how to predict LLM token usage with the MockLLMPredictor class, see below.\n# We then show how to also predict embedding token usage.\n\n# In[ ]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# ## Using MockLLMPredictor\n\n# #### Predicting Usage of GPT Tree Index\n# \n# Here we predict usage of GPTTreeIndex during index construction and querying, without making any LLM calls.\n# \n# NOTE: Predicting query usage before tree is built is only possible with GPTTreeIndex due to the nature of tree traversal. Results will be more accurate if GPTTreeIndex is actually built beforehand.\n\n# In[6]:\n\n\nfrom llama_index import GPTTreeIndex, MockLLMPredictor, SimpleDirectoryReader, ServiceContext\n\n\n# In[7]:\n\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[8]:\n\n\nllm_predictor = MockLLMPredictor(max_tokens=256)\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n\n# In[ ]:\n\n\nindex = GPTTreeIndex.from_documents(documents, service_context=service_context)\n\n\n# In[10]:\n\n\nprint(llm_predictor.last_token_usage)\n\n\n# In[ ]:\n\n\n# default query\nresponse = index.query(\"What did the author do growing up?\", service_context=service_context)\n\n\n# In[13]:\n\n\nprint(llm_predictor.last_token_usage)\n\n\n# #### Predicting Usage of GPT Keyword Table Index Query\n# \n# Here we build a real keyword table index over the data, but then predict query usage.\n\n# In[15]:\n\n\nfrom llama_index import GPTKeywordTableIndex, MockLLMPredictor, SimpleDirectoryReader\n\n\n# In[16]:\n\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = GPTKeywordTableIndex.load_from_disk('../paul_graham_essay/index_table.json')\n\n\n# In[17]:\n\n\nllm_predictor = MockLLMPredictor(max_tokens=256)\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\nresponse = index.query(\"What did the author do after his time at Y Combinator?\", service_context=service_context)\nprint(llm_predictor.last_token_usage)\n\n\n# #### Predicting Usage of GPT List Index Query\n# \n# Here we build a real list index over the data, but then predict query usage.\n\n# In[18]:\n\n\nfrom llama_index import GPTListIndex, MockLLMPredictor, SimpleDirectoryReader\n\n\n# In[19]:\n\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = GPTListIndex.load_from_disk('../paul_graham_essay/index_list.json')\n\n\n# In[20]:\n\n\nllm_predictor = MockLLMPredictor(max_tokens=256)\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\nresponse = index.query(\"What did the author do after his time at Y Combinator?\", service_context=service_context)\n\n\n# In[21]:\n\n\nprint(llm_predictor.last_token_usage)\n\n\n# ## Using MockEmbedding\n\n# #### Predicting Usage of GPT Simple Vector Index\n\n# In[3]:\n\n\nfrom llama_index import GPTSimpleVectorIndex, MockLLMPredictor, MockEmbedding, SimpleDirectoryReader\n\n\n# In[5]:\n\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = GPTSimpleVectorIndex.load_from_disk('../paul_graham_essay/index_simple_vec.json')\n\n\n# In[7]:\n\n\nllm_predictor = MockLLMPredictor(max_tokens=256)\nembed_model = MockEmbedding(embed_dim=1536)\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)\nresponse = index.query(\n    \"What did the author do after his time at Y Combinator?\",\n    service_context=service_context,\n)\n\n", "examples/query_transformations/HyDEQueryTransformDemo.py": "examples/query_transformations/HyDEQueryTransformDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # HyDE Query Transform Demo\n\n# #### Load documents, build the GPTSimpleVectorIndex\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\nfrom llama_index.indices.query.query_transform import HyDEQueryTransform\nfrom IPython.display import Markdown, display\n\n\n# In[ ]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[ ]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\n# save index to disk\nindex.save_to_disk('index_simple.json')\n\n\n# In[ ]:\n\n\n# load index from disk\nindex = GPTSimpleVectorIndex.load_from_disk('index_simple.json')\n\n\n# ## Example: HyDE improves specific temporal queries\n\n# In[ ]:\n\n\nquery_str = \"what did paul graham do after going to RISD\"\n\n\n# #### First, we query *without* transformation: The same query string is used for embedding lookup and also summarization.\n\n# In[ ]:\n\n\nresponse = index.query(query_str)\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# > After going to RISD, Paul Graham continued to pursue his passion for painting and art. He took classes in the painting department at the Accademia di Belli Arti in Florence, and he also took the entrance exam for the school. He also continued to work on his book On Lisp, and he took on consulting work to make money. At the school, Paul Graham and the other students had an arrangement where the faculty wouldn't require the students to learn anything, and in return the students wouldn't require the faculty to teach anything. Paul Graham was one of the few students who actually painted the nude model that was provided, while the rest of the students spent their time chatting or occasionally trying to imitate things they'd seen in American art magazines. The model turned out to live just down the street from Paul Graham, and she made a living from a combination of modelling and making fakes for a local antique dealer.\n\n# #### Now, we use `HyDEQueryTransform` to generate a hypothetical document and use it for embedding lookup. \n\n# In[ ]:\n\n\nhyde = HyDEQueryTransform(include_original=True)\nresponse = index.query(query_str, query_transform=hyde)\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# > After going to RISD, Paul Graham worked as a consultant for Interleaf and then co-founded Viaweb with Robert Morris. They created a software that allowed users to build websites via the web and received $10,000 in seed funding from Idelle's husband Julian. They gave Julian 10% of the company in return for the initial legal work and business advice. Paul Graham had a negative net worth due to taxes he owed, so the seed funding was necessary for him to live on. They opened for business in January 1996 with 6 stores.\n# \n# > Paul Graham then left Yahoo after his options vested and went back to New York. He resumed his old life, but now he was rich. He tried to paint, but he didn't have much energy or ambition. He eventually moved back to Cambridge and started working on a web app for making web apps. He recruited Dan Giffin and two undergrads to help him, but he eventually realized he didn't want to run a company and decided to build a subset of the project as an open source project. He and Dan worked on a new dialect of Lisp, which he called Arc, in a house he bought in Cambridge. The subset he built as an open source project was the new Lisp, whose\n\n# #### In this example, `HyDE` improves output quality significantly, by hallucinating accurately what Paul Graham did after RISD (see below), and thus improving the embedding quality, and final output.\n\n# In[ ]:\n\n\nquery_bundle = hyde(query_str)\nhyde_doc = query_bundle.embedding_strs[0]\n\n\n# In[ ]:\n\n\nhyde_doc\n\n\n# > After graduating from the Rhode Island School of Design (RISD) in 1985, Paul Graham went on to pursue a career in computer programming. He worked as a software developer for several companies, including Viaweb, which he co-founded in 1995. Viaweb was eventually acquired by Yahoo in 1998, and Graham used the proceeds to become a venture capitalist. He founded Y Combinator in 2005, a startup accelerator that has helped launch over 2,000 companies, including Dropbox, Airbnb, and Reddit. Graham has also written several books on programming and startups, and he continues to be an active investor in the tech industry.\n\n# ## Failure case 1: HyDE may mislead when query can be mis-interpreted without context.\n\n# In[ ]:\n\n\nquery_str = \"What is Bel?\"\n\n\n# ### Querying without transformation yields reasonable answer\n\n# In[ ]:\n\n\nresponse = index.query(query_str)\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# > Bel is a programming language that was written in Arc by Paul Graham over the course of four years (March 26, 2015 to October 12, 2019). It is based on John McCarthy's original Lisp, but with additional features added. It is a spec expressed as code, and is meant to be a formal model of computation, an alternative to the Turing machine.\n\n# #### Querying with `HyDEQueryTransform` results in nonsense\n\n# In[ ]:\n\n\nhyde = HyDEQueryTransform(include_original=False)\nresponse = index.query(query_str, query_transform=hyde)\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# > Bel is the pseudonym of Paul Graham, the author of the context information who was in need of seed funding to live on and was part of a deal that became the model for Y Combinator's.\n\n# #### In this example, `HyDE` mis-interprets Bel without document context (see below), resulting in a completely unrelated embedding string and poor retrieval outcome.\n\n# In[ ]:\n\n\nquery_bundle = hyde(query_str)\nhyde_doc = query_bundle.embedding_strs[0]\n\n\n# In[ ]:\n\n\nhyde_doc\n\n\n# > Bel is an ancient Semitic god, originating from the Middle East. He is often associated with the sun and is sometimes referred to as the \"Lord of Heaven\". Bel is also known as the god of fertility, abundance, and prosperity. He is often depicted as a bull or a man with a bull\\'s head. In some cultures, Bel is seen as a creator god, responsible for the creation of the universe. He is also associated with the underworld and is sometimes seen as a god of death. Bel is also associated with justice and is often seen as a protector of the innocent. Bel is an important figure in many religions, including Judaism, Christianity, and Islam.\n\n# ## Failure case 2: HyDE may bias open-ended queries\n\n# In[ ]:\n\n\nquery_str = \"What would the author say about art vs. engineering?\"\n\n\n# #### Querying without transformation yields a reasonable answer\n\n# In[ ]:\n\n\nresponse = index.query(query_str)\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# > The author would likely say that art and engineering are two different disciplines that require different skills and approaches. Art is more focused on expression and creativity, while engineering is more focused on problem-solving and technical knowledge. The author also suggests that art school does not always provide the same level of rigor as engineering school, and that painting students are often encouraged to develop a signature style rather than learn the fundamentals of painting. Furthermore, the author would likely point out that engineering can provide more financial stability than art, as evidenced by the author's own experience of needing seed funding to live on while launching a company.\n\n# #### Querying with `HyDEQueryTransform` results in a more biased output\n\n# In[ ]:\n\n\nhyde = HyDEQueryTransform(include_original=False)\nresponse = index.query(query_str, query_transform=hyde)\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# > The author would likely say that art is a more lasting and independent form of work than engineering. They mention that software written today will be obsolete in a couple decades, and that systems work does not last. In contrast, they note that paintings can last hundreds of years and that it is possible to make a living as an artist. They also mention that as an artist, you can be truly independent and don't need to have a boss or research funding. Furthermore, they note that art can be a source of income for people who may not have access to traditional forms of employment, such as the model in the example who was able to make a living from modelling and making fakes for a local antique dealer.\n", "examples/gatsby/TestGatsby.py": "examples/gatsby/TestGatsby.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# In[6]:\n\n\nfrom llama_index import GPTTreeIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[2]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTTreeIndex.from_documents(documents)\n\n\n# In[3]:\n\n\nindex.save_to_disk('index_gatsby.json')\n\n\n# In[4]:\n\n\n# try loading\nnew_index = GPTTreeIndex.load_from_disk('index_gatsby.json')\n\n\n# In[7]:\n\n\n# set Logging to DEBUG for more detailed outputs\n\nresponse = new_index.query(\"What did the narrator do after getting back to Chicago?\")\n\n\n# In[8]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n# GPT is confused by the text evidence\nresponse = new_index.query(\"What did Gatsby do before he met Daisy?\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/multimodal/Multimodal.py": "examples/multimodal/Multimodal.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[13]:\n\n\nfrom gpt_index import SimpleDirectoryReader, GPTSimpleVectorIndex\nfrom gpt_index.readers.file.base import (\n    DEFAULT_FILE_EXTRACTOR, \n    ImageParser,\n)\nfrom gpt_index.response.notebook_utils import (\n    display_response, \n    display_image,\n)\nfrom gpt_index.indices.query.query_transform.base import (\n    ImageOutputQueryTransform,\n)\n\n\n# In[14]:\n\n\n# NOTE: By default, image parser converts image into text and discard the original image.  \n#       Here, we explicitly keep both the original image and parsed text in an image document\nimage_parser = ImageParser(keep_image=True, parse_text=True)\nfile_extractor = DEFAULT_FILE_EXTRACTOR\nfile_extractor.update(\n{\n    \".jpg\": image_parser,\n    \".png\": image_parser,\n    \".jpeg\": image_parser,\n})\n\n# NOTE: we add filename as metadata for all documents\nfilename_fn = lambda filename: {'file_name': filename}\n\n\n# # Q&A over Receipt Images\n\n# We first ingest our receipt images with the *custom* `image parser` and `metadata function` defined above.   \n# This gives us `image documents` instead of only text documents.\n\n# In[29]:\n\n\nreceipt_reader = SimpleDirectoryReader(\n    input_dir='data/receipts', \n    file_extractor=file_extractor, \n    file_metadata=filename_fn,\n)\nreceipt_documents = receipt_reader.load_data()\n\n\n# We build a simple vector index as usual, but unlike before, our index holds images in addition to text.\n\n# In[30]:\n\n\nreceipts_index = GPTSimpleVectorIndex.from_documents(receipt_documents)\n\n\n# We can now ask a question that prompts for response with both text and image.  \n# We use a custom query transform `ImageOutputQueryTransform` to add instruction on how to display the image nicely in the notebook.\n\n# In[33]:\n\n\nreceipts_response = receipts_index.query(\n    'When was the last time I went to McDonald\\'s and how much did I spend. \\\n    Also show me the receipt from my visit.',\n    query_transform=ImageOutputQueryTransform(width=400)\n)\n\n\n# We now have rich multimodal response with inline text and image!  \n# \n# The source nodes section gives additional details on retrieved data used for synthesizing the final response.  \n# In this case, we can verify that the receipt for McDonald's is correctly retrieved. \n\n# In[34]:\n\n\ndisplay_response(receipts_response)\n\n\n# # Q & A over LlamaIndex Documentation\n\n# We now demo the same for Q&A over LlamaIndex documentations.   \n# This demo higlights the ability to synthesize multimodal output with a mixture of text and image documents\n\n# In[15]:\n\n\nllama_reader = SimpleDirectoryReader(\n    input_dir='data/llama',\n    file_extractor=file_extractor, \n    file_metadata=filename_fn,\n)\nllama_documents = llama_reader.load_data(concatenate=True)\n\n\n# In[16]:\n\n\nllama_index = GPTSimpleVectorIndex.from_documents(llama_documents)\n\n\n# In[19]:\n\n\nllama_response = llama_index.query(\n    'Show an image to illustrate how tree index works and explain briefly.', \n    query_transform=ImageOutputQueryTransform(width=400),\n    similarity_top_k=2\n)\n\n\n# By inspecting the 2 source nodes, we see relevant text and image describing the tree index are retrieved for synthesizing the final multimodal response.\n\n# In[20]:\n\n\ndisplay_response(llama_response)\n\n\n# We show another example asking about vector store index instead.\n\n# In[17]:\n\n\nllama_response = llama_index.query(\n    'Show an image to illustrate how vector store index works and explain briefly.', \n    query_transform=ImageOutputQueryTransform(width=400),\n    similarity_top_k=2\n)\n\n\n# In[18]:\n\n\ndisplay_response(llama_response)\n\n", "examples/vector_indices/FaissIndexDemo.py": "examples/vector_indices/FaissIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Faiss Index Demo\n\n# #### Creating a Faiss Index\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[3]:\n\n\nimport faiss\n# dimensions of text-ada-embedding-002\nd = 1536 \nfaiss_index = faiss.IndexFlatL2(d)\n\n\n# #### Load documents, build the GPTFaissIndex\n\n# In[4]:\n\n\nfrom gpt_index import GPTFaissIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[5]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[ ]:\n\n\nindex = GPTFaissIndex.from_documents(documents, faiss_index=faiss_index)\n\n\n# In[10]:\n\n\n# save index to disk\nindex.save_to_disk(\n    'index_faiss.json', \n    faiss_index_save_path=\"index_faiss_core.index\"\n)\n\n\n# In[11]:\n\n\n# load index from disk\nindex = GPTFaissIndex.load_from_disk(\n    'index_faiss.json', \n    faiss_index_save_path=\"index_faiss_core.index\"\n)\n\n\n# #### Query Index\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do growing up?\")\n\n\n# In[13]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[7]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do after his time at Y Combinator?\")\n\n\n# In[8]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n\n\n", "examples/vector_indices/QdrantIndexDemo.py": "examples/vector_indices/QdrantIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Qdrant Index Demo\n\n# #### Creating a Qdrant client\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[2]:\n\n\nimport qdrant_client\n\n\n# In[3]:\n\n\nclient = qdrant_client.QdrantClient(\n    host=\"<qdrant-host>\",\n    api_key=\"<qdrant-api-key>\", # For Qdrant Cloud, None for local instance\n    https=True, # True if using Qdrant Cloud\n)\n\n\n# #### Load documents, build the GPTQdrantIndex\n\n# In[4]:\n\n\nfrom gpt_index import GPTQdrantIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[5]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[6]:\n\n\nindex = GPTQdrantIndex.from_documents(documents, client=client, collection_name=\"paul_graham\")\n\n\n# In[7]:\n\n\n# save index to disk\nindex.save_to_disk(\"index_qdrant.json\")\n\n\n# In[8]:\n\n\n# load index from disk\nindex = GPTQdrantIndex.load_from_disk(\"index_qdrant.json\", client=client)\n\n\n# #### Query Index\n\n# In[9]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do growing up?\")\n\n\n# In[10]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[11]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do after his time at Y Combinator?\")\n\n\n# In[12]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n\n\n", "examples/vector_indices/SimpleIndexDemo-streaming.py": "examples/vector_indices/SimpleIndexDemo-streaming.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Simple Index Demo\n\n# #### Load documents, build the GPTSimpleVectorIndex\n\n# In[3]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[4]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[12]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents, chunk_size_limit=1024)\n\n\n# #### Query Index\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse_stream = index.query(\n    \"What did the author do growing up?\", \n    streaming=True,\n    similarity_top_k=1\n)\n\n\n# In[ ]:\n\n\nresponse_stream.print_response_stream()\n\n\n# In[ ]:\n\n\n# can also get a normal response object\nresponse = response_stream.get_response()\n\n", "examples/vector_indices/PineconeIndexDemo.py": "examples/vector_indices/PineconeIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Pinecone Index Demo\n\n# #### Creating a Pinecone Index\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[1]:\n\n\nimport pinecone\n\n\n# In[2]:\n\n\napi_key = \"api_key\"\npinecone.init(api_key=api_key, environment=\"us-east1-gcp\")\n\n\n# In[ ]:\n\n\n# dimensions are for text-embedding-ada-002\npinecone.create_index(\"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")\n\n\n# In[4]:\n\n\npinecone_index = pinecone.Index(\"quickstart\")\n\n\n# #### Load documents, build the GPTPineconeIndex\n\n# In[5]:\n\n\nfrom gpt_index import GPTPineconeIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[6]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[ ]:\n\n\n# initialize without metadata filter\nindex = GPTPineconeIndex(documents, pinecone_index=pinecone_index)\n\n\n# In[ ]:\n\n\n# [optional] initialize with metadata filters\n# can define filters specific to this vector index (so you can\n# reuse pinecone indexes)\nmetadata_filters = {\"title\": \"paul_graham_essay\"}\n\nindex = GPTPineconeIndex(documents, pinecone_index=pinecone_index, metadata_filters=metadata_filters)\n\n\n# #### Query Index\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do growing up?\")\n\n\n# In[9]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/vector_indices/SimpleIndexDemo-ChatGPT.py": "examples/vector_indices/SimpleIndexDemo-ChatGPT.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Simple Index Demo + ChatGPT\n\n# Use a very simple wrapper around the ChatGPT API\n\n# #### Load documents, build the GPTSimpleVectorIndex\n\n# In[7]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\nfrom langchain.chat_models import ChatOpenAI\nfrom IPython.display import Markdown, display\n\n\n# In[8]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[ ]:\n\n\n# LLM Predictor (gpt-3.5-turbo) + service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n\n\n# In[ ]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n\n\n# #### Query Index\n\n# By default, with the help of langchain's PromptSelector abstraction, we use \n# a modified refine prompt tailored for ChatGPT-use if the ChatGPT model is used.\n\n# In[5]:\n\n\nresponse = index.query(\n    \"What did the author do growing up?\", \n    service_context=service_context,\n    similarity_top_k=3\n)\n\n\n# In[6]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\nresponse = index.query(\n    \"What did the author do during his time at RISD?\", \n    service_context=service_context,\n    similarity_top_k=5\n)\n\n\n# In[9]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# **Refine Prompt**: Here is the chat refine prompt \n\n# In[11]:\n\n\nfrom gpt_index.prompts.chat_prompts import CHAT_REFINE_PROMPT\n\n\n# In[22]:\n\n\ndict(CHAT_REFINE_PROMPT.prompt)\n\n\n# #### Query Index (Using the standard Refine Prompt)\n# \n# If we use the \"standard\" refine prompt (where the prompt is one text template instead of multiple messages), we find that the results over ChatGPT are worse. \n\n# In[27]:\n\n\nfrom gpt_index.prompts.default_prompts import DEFAULT_REFINE_PROMPT\n\n\n# In[ ]:\n\n\nresponse = index.query(\n    \"What did the author do during his time at RISD?\", \n    service_context=service_context,\n    refine_template=DEFAULT_REFINE_PROMPT,\n    similarity_top_k=5\n)\n\n\n# In[31]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n", "examples/vector_indices/AsyncIndexCreationDemo.py": "examples/vector_indices/AsyncIndexCreationDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Async Index Creation Demo\n\n# In[1]:\n\n\nimport time\n\n# Helps asyncio run within Jupyter\nimport nest_asyncio\nnest_asyncio.apply()\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"[YOUR_API_KEY]\"\n\n\n# In[4]:\n\n\nfrom llama_index import GPTSimpleVectorIndex, download_loader\n\nWikipediaReader = download_loader(\"WikipediaReader\")\n\nloader = WikipediaReader()\ndocuments = loader.load_data(pages=['Berlin', 'Santiago', 'Moscow', 'Tokyo', 'Jakarta', 'Cairo', 'Bogota', 'Shanghai', 'Damascus'])\n\n\n# In[5]:\n\n\nlen(documents)\n\n\n# 9 Wikipedia articles downloaded as documents\n\n# In[11]:\n\n\nstart_time = time.perf_counter()\nindex = GPTSimpleVectorIndex.from_documents(documents)\nduration = time.perf_counter() - start_time\nprint(duration)\n\n\n# Standard index creation took 7.69 seconds\n\n# In[10]:\n\n\nstart_time = time.perf_counter()\nindex = GPTSimpleVectorIndex(documents, use_async=True)\nduration = time.perf_counter() - start_time\nprint(duration)\n\n\n# Async index creation took 2.37 seconds\n\n# In[8]:\n\n\nindex.query(\"What is the etymology of Jakarta?\")\n\n\n# In[ ]:\n\n\n\n\n", "examples/vector_indices/SimpleIndexDemo-multistep.py": "examples/vector_indices/SimpleIndexDemo-multistep.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Simple Index Demo\n\n# #### Load documents, build the GPTSimpleVectorIndex\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom gpt_index import (\n    GPTSimpleVectorIndex, \n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext\n)\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\nfrom IPython.display import Markdown, display\n\n\n# In[2]:\n\n\n# LLM Predictor (gpt-3)\nllm_predictor_gpt3 = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\nservice_context_gpt3 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt3)\n\n# LLMPredictor (gpt-4)\nllm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context_gpt4 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt4)\n\n\n# In[3]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[4]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents)\n\n\n# In[5]:\n\n\n# save index to disk\nindex.save_to_disk('index_simple.json')\n\n\n# In[6]:\n\n\n# load index from disk\nindex = GPTSimpleVectorIndex.load_from_disk('index_simple.json')\n\n\n# #### Query Index\n\n# In[7]:\n\n\nfrom gpt_index.indices.query.query_transform.base import StepDecomposeQueryTransform\n# gpt-4\nstep_decompose_transform = StepDecomposeQueryTransform(\n    llm_predictor_gpt4, verbose=True\n)\n\n# gpt-3\nstep_decompose_transform_gpt3 = StepDecomposeQueryTransform(\n    llm_predictor_gpt3, verbose=True\n)\n\n\n# In[8]:\n\n\nindex.index_struct.summary = \"Used to answer questions about the author\"\n\n\n# In[9]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse_gpt4 = index.query(\n    \"Who was in the first batch of the accelerator program the author started?\",\n    query_transform=step_decompose_transform,\n    service_context=service_context_gpt4\n)\n\n\n# In[10]:\n\n\ndisplay(Markdown(f\"<b>{response_gpt4}</b>\"))\n\n\n# In[11]:\n\n\nsub_qa = response_gpt4.extra_info[\"sub_qa\"]\ntuples = [(t[0], t[1].response) for t in sub_qa]\nprint(tuples)\n\n\n# In[13]:\n\n\nresponse_gpt4 = index.query(\n    \"In which city did the author found his first company, Viaweb?\",\n    query_transform=step_decompose_transform,\n    service_context=service_context_gpt4\n)\n\n\n# In[14]:\n\n\nprint(response_gpt4)\n\n\n# In[15]:\n\n\nresponse_gpt3 = index.query(\n    \"In which city did the author found his first company, Viaweb?\",\n    query_transform=step_decompose_transform_gpt3,\n    service_context=service_context_gpt3\n)\n\n\n# In[16]:\n\n\nprint(response_gpt3)\n\n\n# In[ ]:\n\n\n\n\n", "examples/vector_indices/WeaviateIndexDemo.py": "examples/vector_indices/WeaviateIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Weaviate Index Demo\n\n# #### Creating a Weaviate Client\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[1]:\n\n\nimport weaviate\n\n\n# In[2]:\n\n\nresource_owner_config = weaviate.AuthClientPassword(\n  username = \"<username>\", \n  password = \"<password>\", \n)\n\n\n# In[3]:\n\n\nclient = weaviate.Client(\"https://<cluster-id>.semi.network/\", auth_client_secret=resource_owner_config)\n\n\n# #### Load documents, build the GPTWeaviateIndex\n\n# In[ ]:\n\n\nfrom gpt_index import GPTWeaviateIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[6]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[ ]:\n\n\nindex = GPTWeaviateIndex.from_documents(documents, weaviate_client=client)\n\n# NOTE: you may also choose to define a class_prefix manually.\n# class_prefix = \"test_prefix\"\n# index = GPTWeaviateIndex.from_documents(documents, weaviate_client=client, class_prefix=class_prefix)\n\n\n# In[10]:\n\n\n# save index to disk\nindex.save_to_disk('index_weaviate.json')\n\n\n# In[11]:\n\n\n# load index from disk\nindex = GPTWeaviateIndex.load_from_disk('index_weaviate.json', weaviate_client=client)\n\n\n# #### Query Index\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do growing up?\")\n\n\n# In[13]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/vector_indices/ChromaIndexDemo.py": "examples/vector_indices/ChromaIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Chroma Index Demo\n\n# #### Creating a Chroma Index\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nimport chromadb\n\n\n# In[ ]:\n\n\nchroma_client = chromadb.Client()\nchroma_collection = chroma_client.create_collection(\"quickstart\")\n\n\n# #### Load documents, build the GPTChromaIndex\n\n# In[ ]:\n\n\nfrom gpt_index import GPTChromaIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[ ]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[ ]:\n\n\nindex = GPTChromaIndex.from_documents(documents, chroma_collection=chroma_collection)\n\n\n# #### Query Index\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do growing up?\", chroma_collection=chroma_collection)\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/vector_indices/OpensearchDemo.py": "examples/vector_indices/OpensearchDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Using as a vector index.\n# \n# Elasticsearch only supports Lucene indices, so only Opensearch is supported.\n\n# **Note on setup**: We setup a local Opensearch instance through the following doc. https://opensearch.org/docs/1.0/\n# \n# If you run into SSL issues, try the following `docker run` command instead: \n# ```\n# docker run -p 9200:9200 -p 9600:9600 -e \"discovery.type=single-node\" -e \"plugins.security.disabled=true\" opensearchproject/opensearch:1.0.1\n# ```\n# \n# Reference: https://github.com/opensearch-project/OpenSearch/issues/1598\n\n# In[ ]:\n\n\nfrom os import getenv\nfrom llama_index import SimpleDirectoryReader\nfrom llama_index.indices.vector_store import GPTOpensearchIndex\nfrom llama_index.vector_stores import OpensearchVectorClient\n# http endpoint for your cluster (opensearch required for vector index usage)\nendpoint = getenv(\"OPENSEARCH_ENDPOINT\", \"http://localhost:9200\")\n# index to demonstrate the VectorStore impl\nidx = getenv(\"OPENSEARCH_INDEX\", \"gpt-index-demo\")\n# load some sample data\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[3]:\n\n\n# OpensearchVectorClient stores text in this field by default\ntext_field = \"content\"\n# OpensearchVectorClient stores embeddings in this field by default\nembedding_field = \"embedding\"\n# OpensearchVectorClient encapsulates logic for a\n# single opensearch index with vector search enabled\nclient = OpensearchVectorClient(endpoint, idx, 1536, embedding_field=embedding_field, text_field=text_field)\n# initialize an index using our sample data and the client we just created\nindex = GPTOpensearchIndex.from_documents(documents=documents, client=client)\n\n\n# In[4]:\n\n\n# run query\nres = index.query(\"What did the author do growing up?\")\nres.response\n\n\n# ## Use reader to check out what GPTOpensearchIndex just created in our index.\n# \n# Reader works with Elasticsearch too as it just uses the basic search features.\n\n# In[9]:\n\n\n# create a reader to check out the index used in previous section.\nfrom llama_index.readers import ElasticsearchReader\n\nrdr = ElasticsearchReader(endpoint, idx)\n# set embedding_field optionally to read embedding data from the elasticsearch index\ndocs = rdr.load_data(text_field, embedding_field=embedding_field)\n# docs have embeddings in them\nprint(\"embedding dimension:\", len(docs[0].embedding))\n# full document is stored in extra_info\nprint(\"all fields in index:\", docs[0].extra_info.keys())\n\n\n# In[10]:\n\n\n# we can check out how the text was chunked by the `GPTOpensearchIndex`\nprint(\"total number of chunks created:\", len(docs))\n\n\n# In[13]:\n\n\n# search index using standard elasticsearch query DSL\ndocs = rdr.load_data(text_field, {\"query\": {\"match\": {text_field: \"Lisp\"}}})\nprint(\"chunks that mention Lisp:\", len(docs))\ndocs = rdr.load_data(text_field, {\"query\": {\"match\": {text_field: \"Yahoo\"}}})\nprint(\"chunks that mention Yahoo:\", len(docs))\n\n", "examples/vector_indices/SimpleIndexDemo.py": "examples/vector_indices/SimpleIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Simple Index Demo\n\n# #### Load documents, build the GPTSimpleVectorIndex\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[2]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[3]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents)\n\n\n# In[4]:\n\n\n# save index to disk\nindex.save_to_disk('index_simple.json')\n\n\n# In[5]:\n\n\n# load index from disk\nindex = GPTSimpleVectorIndex.load_from_disk('index_simple.json')\n\n\n# #### Query Index\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do growing up?\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# **Query Index with custom embedding string**\n\n# In[ ]:\n\n\nfrom gpt_index.indices.query.schema import QueryBundle\n\n\n# In[ ]:\n\n\nquery_bundle = QueryBundle(\n    query_str=\"What did the author do growing up?\", \n    custom_embedding_strs=['The author grew up painting.']\n)\nresponse = index.query(query_bundle)\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# #### Get Sources\n\n# In[ ]:\n\n\nprint(response.get_formatted_sources())\n\n\n# #### Query Index with LlamaLogger\n# \n# Log intermediate outputs and view/use them.\n\n# In[6]:\n\n\nfrom gpt_index.logger import LlamaLogger\nfrom gpt_index import ServiceContext\n\nllama_logger = LlamaLogger()\nservice_context = ServiceContext.from_defaults(llama_logger=llama_logger)\n\n\n# In[ ]:\n\n\nresponse = index.query(\n    \"What did the author do growing up?\",\n    service_context=service_context,\n    similarity_top_k=2,\n    # response_mode=\"tree_summarize\"\n)\n\n\n# In[ ]:\n\n\n# get logs\nservice_context.llama_logger.get_logs()\n\n\n# In[ ]:\n\n\n\n\n", "examples/paul_graham_essay/TestEssay.py": "examples/paul_graham_essay/TestEssay.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# ## Using GPT Tree Index\n\n# #### [Demo] Default leaf traversal \n\n# In[ ]:\n\n\nfrom llama_index import GPTTreeIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[2]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\n\n# In[ ]:\n\n\nindex = GPTTreeIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\nindex.save_to_disk('index.json')\n\n\n# In[2]:\n\n\n# try loading\nnew_index = GPTTreeIndex.load_from_disk('index.json')\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = new_index.query(\"What did the author do growing up?\")\n\n\n# In[4]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = new_index.query(\"What did the author do after his time at Y Combinator?\")\n\n\n# In[6]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# #### [Demo] Leaf traversal with child_branch_factor=2\n\n# In[ ]:\n\n\n# try using branching factor 2\nresponse = new_index.query(\"What did the author do growing up?\", child_branch_factor=2)\n\n\n# In[6]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# #### [Demo] Build Tree Index during Query-Time\n\n# In[3]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\n\n# In[4]:\n\n\nindex_light = GPTTreeIndex.from_documents(documents, build_tree=False)\n\n\n# In[7]:\n\n\nindex_light.query(\"What did the author do after his time at Y Combinator?\", mode=\"summarize\")\n\n\n# #### [Demo] Build Tree Index with a custom Summary Prompt, directly retrieve answer from root node\n\n# In[20]:\n\n\nfrom llama_index import SummaryPrompt\n\n\n# In[23]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\nquery_str = \"What did the author do growing up?\"\nSUMMARY_PROMPT_TMPL = (\n    \"Context information is below. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    f\"answer the question: {query_str}\\n\"\n)\nSUMMARY_PROMPT = SummaryPrompt(SUMMARY_PROMPT_TMPL)\nindex_with_query = GPTTreeIndex.from_documents(documents, summary_template=SUMMARY_PROMPT)\n\n\n# In[6]:\n\n\nindex_with_query.save_to_disk(\"index_with_query.json\")\n\n\n# In[7]:\n\n\nindex_with_query = GPTTreeIndex.load_from_disk(\"index_with_query.json\")\n\n\n# In[ ]:\n\n\n# directly retrieve response from root nodes instead of traversing tree\nresponse = index_with_query.query(query_str, mode=\"retrieve\")\n\n\n# In[10]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# ## Using GPT Keyword Table Index\n\n# In[7]:\n\n\nfrom llama_index import GPTKeywordTableIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[2]:\n\n\n# build keyword index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTKeywordTableIndex.from_documents(documents)\n\n\n# In[4]:\n\n\n# save index\nindex.save_to_disk('index_table.json')\n\n\n# In[8]:\n\n\n# reload index\nindex = GPTKeywordTableIndex.load_from_disk('index_table.json')\n\n\n# In[9]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do after his time at Y Combinator?\")\n\n\n# In[10]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# ## Using GPT List Index\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[2]:\n\n\n# build linked list index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTListIndex.from_documents(documents)\n# save index\nindex.save_to_disk('index_list.json')\n\n\n# In[14]:\n\n\n# load index from disk\nindex = GPTListIndex.load_from_disk('index_list.json')\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do after his time at Y Combinator?\")\n\n\n# In[16]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n\n\n", "examples/paul_graham_essay/GPT4Comparison.py": "examples/paul_graham_essay/GPT4Comparison.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[62]:\n\n\nfrom gpt_index import GPTListIndex, SimpleDirectoryReader, LLMPredictor, PromptHelper, ServiceContext\nfrom gpt_index.response.notebook_utils import display_response\nfrom langchain import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom IPython.display import Markdown, display\n\n\n# In[3]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\n\n# # davinci-003\n\n# In[34]:\n\n\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n\n# In[35]:\n\n\ndavinci_index = GPTListIndex.from_documents(documents, service_context=service_context)\n\n\n# In[45]:\n\n\nf'Document is split into {len(davinci_index._index_struct.nodes)} nodes.'\n\n\n# In[68]:\n\n\nresponse = davinci_index.query(\n    \"What happened on one night in October 2003?\", \n    response_mode=\"tree_summarize\"\n)\n\n\n# In[69]:\n\n\ndisplay_response(response)\n\n\n# # gpt-4\n\n# In[63]:\n\n\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n\n# In[64]:\n\n\ngpt4_index = GPTListIndex.from_documents(documents, service_context=service_context)\n\n\n# In[65]:\n\n\nf'Document is split into {len(gpt4_index._index_struct.nodes)} nodes.'\n\n\n# In[70]:\n\n\nresponse = gpt4_index.query(\n    \"What happened on one night in October 2003?\", \n    response_mode=\"tree_summarize\"\n)\n\n\n# In[71]:\n\n\ndisplay_response(response)\n\n\n# # gpt-4-32k\n\n# NOTE: not available yet\n\n# In[ ]:\n\n\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-32k\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n\n# In[ ]:\n\n\ngpt4_32k_index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n\n\n# In[ ]:\n\n\nlen(gpt4_32k_index._index_struct.nodes_dict)\n\n", "examples/paul_graham_essay/KeywordTableComparison.py": "examples/paul_graham_essay/KeywordTableComparison.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# ## GPT Keyword Table Index Comparisons\n# \n# Comparing GPTSimpleKeywordTableIndex, GPTRAKEKeywordTableIndex, GPTKeywordTableIndex.\n# \n# - GPTSimpleKeywordTableIndex - uses simple regex to extract keywords.\n# - GPTRAKEKeywordTableIndex - uses RAKE to extract keywords.\n# - GPTKeywordTableIndex - uses GPT to extract keywords.\n\n# #### GPTSimpleKeywordTableIndex\n\n# In[1]:\n\n\nfrom llama_index import GPTSimpleKeywordTableIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[ ]:\n\n\n# build keyword index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTSimpleKeywordTableIndex(documents)\n\n\n# In[ ]:\n\n\nresponse = index.query(\"What did the author do after his time at YC?\")\n\n\n# In[5]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# #### GPTRAKEKeywordTableIndex\n\n# In[1]:\n\n\nfrom llama_index import GPTRAKEKeywordTableIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[ ]:\n\n\n# build keyword index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTRAKEKeywordTableIndex(documents)\n\n\n# In[10]:\n\n\nresponse = index.query(\"What did the author do after his time at YC?\")\n\n\n# In[11]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# #### GPTKeywordTableIndex\n\n# In[7]:\n\n\nfrom llama_index import GPTKeywordTableIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[ ]:\n\n\n# build keyword index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTKeywordTableIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\nresponse = index.query(\"What did the author do after his time at Y Combinator?\")\n\n\n# In[10]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# ## GPT Keyword Table Query Comparisons\n# Compare mode={\"default\", \"simple\", \"rake\"}\n\n# In[ ]:\n\n\n# build table with default GPTKeywordTableIndex\nfrom llama_index import GPTKeywordTableIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTKeywordTableIndex.from_documents(documents)\n\n\n# In[3]:\n\n\n# default\nresponse = index.query(\"What did the author do after his time at Y Combinator?\", mode=\"default\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[4]:\n\n\n# simple\nresponse = index.query(\"What did the author do after his time at Y Combinator?\", mode=\"simple\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[5]:\n\n\n# rake\nresponse = index.query(\"What did the author do after his time at Y Combinator?\", mode=\"rake\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n", "examples/paul_graham_essay/SentenceSplittingDemo.py": "examples/paul_graham_essay/SentenceSplittingDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Example of using sentence splitter chunking\n# Compare the diff of splitting_1.txt and splitting_2.txt\n\n# In[1]:\n\n\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom llama_index import SimpleDirectoryReader, Document\nfrom gpt_index.utils import globals_helper\nfrom langchain.text_splitter import NLTKTextSplitter, SpacyTextSplitter, RecursiveCharacterTextSplitter\n\ndocument = SimpleDirectoryReader('data').load_data()[0]\ntext_splitter_default = TokenTextSplitter() # use default settings\ntext_chunks = text_splitter_default.split_text(document.text)\ndoc_chunks = [Document(t) for t in text_chunks]\ntokenizer = globals_helper.tokenizer\nwith open('splitting_1.txt', 'w') as f:\n    for idx, doc in enumerate(doc_chunks):\n        f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n\nfrom gpt_index.langchain_helpers.text_splitter import SentenceSplitter\n\nsentence_splitter = SentenceSplitter()\ntext_chunks = sentence_splitter.split_text(document.text)\ndoc_chunks = [Document(t) for t in text_chunks]\nwith open('splitting_2.txt', 'w') as f:\n    for idx, doc in enumerate(doc_chunks):\n        f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n\nnltk_splitter = NLTKTextSplitter()\ntext_chunks = nltk_splitter.split_text(document.text)\ndoc_chunks = [Document(t) for t in text_chunks]\ntokenizer = globals_helper.tokenizer\nwith open('splitting_3.txt', 'w') as f:\n    for idx, doc in enumerate(doc_chunks):\n        f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n\n# spacy_splitter = SpacyTextSplitter()\n# text_chunks = spacy_splitter.split_text(document.text)\n# tokenizer = globals_helper.tokenizer\n# with open('splitting_4.txt', 'w') as f:\n#     for idx, doc in enumerate(doc_chunks):\n#         f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n\n# from langchain.text_splitter import TokenTextSplitter\n# token_text_splitter = TokenTextSplitter()\n# text_chunks = token_text_splitter.split_text(document.text)\n# doc_chunks = [Document(t) for t in text_chunks]\n# tokenizer = globals_helper.tokenizer\n# with open('splitting_5.txt', 'w') as f:\n#     for idx, doc in enumerate(doc_chunks):\n#         f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n\n# recursive_splitter = RecursiveCharacterTextSplitter()\n# text_chunks = recursive_splitter.split_text(document.text)\n# doc_chunks = [Document(t) for t in text_chunks]\n# tokenizer = globals_helper.tokenizer\n# with open('splitting_6.txt', 'w') as f:\n#     for idx, doc in enumerate(doc_chunks):\n#         f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n\n", "examples/paul_graham_essay/DavinciComparison.py": "examples/paul_graham_essay/DavinciComparison.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# # Comparing text-davinci-002 vs. text-davinci-003\n# \n# Does text-davinci-003 do better?\n\n# #### text-davinci-002\n\n# In[1]:\n\n\nfrom llama_index import GPTKeywordTableIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\nfrom langchain import OpenAI\nfrom IPython.display import Markdown, display\n\n\n# In[2]:\n\n\n# load index\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\nindex = GPTKeywordTableIndex.load_from_disk('index_table.json', service_context=service_context)\n\n\n# In[ ]:\n\n\nresponse = index.query(\"What did the author do after his time at Y Combinator?\")\n\n\n# In[4]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# #### text-davinci-003\n\n# In[1]:\n\n\nfrom llama_index import GPTKeywordTableIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\nfrom langchain import OpenAI\nfrom IPython.display import Markdown, display\n\n\n# In[4]:\n\n\n# load index\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\nindex = GPTKeywordTableIndex.load_from_disk('index_table.json', service_context=service_context)\n\n\n# In[ ]:\n\n\nresponse = index.query(\"What did the author do after his time at Y Combinator?\")\n\n\n# In[6]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n\n\n", "examples/paul_graham_essay/InsertDemo.py": "examples/paul_graham_essay/InsertDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# This notebook showcases the insert capabilities of different GPT Index data structures.\n# \n# To see how to build the index during initialization, see `TestEssay.ipynb` instead.\n\n# In[ ]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# ## GPT List Insert\n\n# #### Data Prep\n# Chunk up the data into sub documents that we can insert\n\n# In[ ]:\n\n\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom llama_index import SimpleDirectoryReader, Document\n\n\n# In[15]:\n\n\ndocument = SimpleDirectoryReader('data').load_data()[0]\ntext_splitter = TokenTextSplitter(separator=\" \", chunk_size=2048, chunk_overlap=20)\ntext_chunks = text_splitter.split_text(document.text)\ndoc_chunks = [Document(t) for t in text_chunks]\n\n\n# #### Insert into Index and Query\n\n# In[16]:\n\n\nfrom llama_index import GPTListIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[17]:\n\n\n# initialize blank list index\nindex = GPTListIndex([])\n\n\n# In[ ]:\n\n\n# insert new document chunks\nfor doc_chunk in doc_chunks:\n    index.insert(doc_chunk)\n\n\n# In[19]:\n\n\n# query\nresponse = index.query(\"What did the author do growing up?\")\n\n\n# In[21]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# ## GPT Tree Insert\n\n# #### Data Prep\n# Chunk up the data into sub documents that we can insert\n\n# In[2]:\n\n\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom llama_index import SimpleDirectoryReader, Document\n\n\n# In[2]:\n\n\n# NOTE: we truncate to the first 30 nodes to save on cost\ndocument = SimpleDirectoryReader('data').load_data()[0]\ntext_splitter = TokenTextSplitter(separator=\" \", chunk_size=256, chunk_overlap=20)\ntext_chunks = text_splitter.split_text(document.get_text())\ndoc_chunks = [Document(t) for t in text_chunks]\n\ndoc_chunks = doc_chunks[:30]\n\n\n# #### Insert into Index and Query\n\n# In[3]:\n\n\nfrom llama_index import GPTTreeIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[4]:\n\n\n# initialize blank tree index\ntree_index = GPTTreeIndex([])\n\n\n# In[ ]:\n\n\n# insert new document chunks\nfor i, doc_chunk in enumerate(doc_chunks):\n    print(f\"Inserting {i}/{len(doc_chunks)}\")\n    tree_index.insert(doc_chunk)\n\n\n# In[6]:\n\n\ntree_index.save_to_disk('index_tree_insert.json')\n\n\n# In[7]:\n\n\ntree_index = GPTTreeIndex.load_from_disk('index_tree_insert.json')\n\n\n# In[ ]:\n\n\n# query\nresponse_tree = tree_index.query(\"What did the author do growing up?\")\n\n\n# In[9]:\n\n\ndisplay(Markdown(f\"<b>{response_tree}</b>\"))\n\n", "examples/evaluation/LangchainOutputParserDemo.py": "examples/evaluation/LangchainOutputParserDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Simple Index Demo\n\n# #### Load documents, build the GPTSimpleVectorIndex\n\n# In[17]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[2]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[3]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents, chunk_size_limit=512)\n\n\n# In[4]:\n\n\n# save index to disk\nindex.save_to_disk('index_simple.json')\n\n\n# In[18]:\n\n\n# load index from disk\nindex = GPTSimpleVectorIndex.load_from_disk('index_simple.json')\n\n\n# #### Define Query + Langchain Output Parser\n\n# In[2]:\n\n\nfrom gpt_index.output_parsers import LangchainOutputParser\nfrom gpt_index.llm_predictor import StructuredLLMPredictor\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\n\n# In[3]:\n\n\nllm_predictor = StructuredLLMPredictor()\n\n\n# **Define custom QA and Refine Prompts**\n\n# In[4]:\n\n\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\n\n\n# In[5]:\n\n\nresponse_schemas = [\n    ResponseSchema(name=\"Education\", description=\"Describes the author's educational experience/background.\"),\n    ResponseSchema(name=\"Work\", description=\"Describes the author's work experience/background.\")\n]\n\n\n# In[6]:\n\n\nlc_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\noutput_parser = LangchainOutputParser(lc_output_parser)\n\n\n# In[15]:\n\n\n# NOTE: we use the same output parser for both prompts, though you can choose to use different parsers\n# NOTE: here we add formatting instructions to the prompts.\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\n\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\n\n# In[10]:\n\n\n# take a look at the new QA template! \nprint(fmt_qa_tmpl)\n\n\n# #### Query Index\n\n# In[19]:\n\n\nresponse = index.query(\n    \"What are a few things the author did growing up?\", \n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n    llm_predictor=llm_predictor\n)\n\n\n# In[20]:\n\n\nprint(response)\n\n\n# In[ ]:\n\n\n\n\n", "examples/evaluation/TestNYC-Evaluation.py": "examples/evaluation/TestNYC-Evaluation.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[26]:\n\n\nfrom gpt_index import (\n    GPTTreeIndex, \n    GPTSimpleVectorIndex, \n    SimpleDirectoryReader, \n    LLMPredictor, \n    ServiceContext,\n    Response\n)\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\nfrom gpt_index.evaluation import ResponseEvaluator\nimport pandas as pd\npd.set_option('display.max_colwidth', 0)\n\n\n# In[3]:\n\n\n# gpt-3 (davinci)\nllm_predictor_gpt3 = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\nservice_context_gpt3 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt3)\n\n# gpt-4\nllm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context_gpt4 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt4)\n\n\n# In[4]:\n\n\nevaluator = ResponseEvaluator(service_context=service_context_gpt3)\nevaluator_gpt4 = ResponseEvaluator(service_context=service_context_gpt4)\n\n\n# In[32]:\n\n\ndocuments = SimpleDirectoryReader('../test_wiki/data').load_data()\n\n\n# In[5]:\n\n\n# load tree index from stored index\ntree_index = GPTTreeIndex.load_from_disk('../test_wiki/index.json')\n\n\n# In[33]:\n\n\n# create vector index\nvector_index = GPTSimpleVectorIndex.from_documents(\n    documents, \n    service_context=ServiceContext.from_defaults(chunk_size_limit=512)\n)\nvector_index.save_to_disk('../test_wiki/simple_vector_index.json')\n\nvector_index = GPTSimpleVectorIndex.load_from_disk('../test_wiki/simple_vector_index.json')\n\n\n# In[54]:\n\n\n# define jupyter display function\ndef display_eval_df(response: Response, eval_result: str) -> None:\n    eval_df = pd.DataFrame(\n        {\n            \"Response\": str(response), \n            \"Source\": response.source_nodes[0].source_text[:1000] + \"...\",\n            \"Evaluation Result\": eval_result\n        },\n        index=[0]\n    )\n    eval_df = eval_df.style.set_properties(\n        **{\n            'inline-size': '600px',\n            'overflow-wrap': 'break-word',\n        }, \n        subset=[\"Response\", \"Source\"]\n    )\n    display(eval_df)\n\n\n# In[65]:\n\n\nresponse_tree = tree_index.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator_gpt4.evaluate(response_tree)\n\n\n# In[66]:\n\n\ndisplay_eval_df(response_tree, eval_result)\n\n\n# In[ ]:\n\n\nresponse_vector = vector_index.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator_gpt4.evaluate(response_vector)\n\n\n# In[55]:\n\n\ndisplay_eval_df(response_vector, eval_result)\n\n\n# In[ ]:\n\n\nresponse_tree = tree_index.query(\"What are the airports in New York City?\")\neval_result = evaluator_gpt4.evaluate(response_tree)\n\n\n# In[60]:\n\n\ndisplay_eval_df(response_tree, eval_result)\n\n\n# In[ ]:\n\n\nresponse_vector = vector_index.query(\"What are the airports in New York City?\")\neval_result = evaluator_gpt4.evaluate(response_vector)\n\n\n# In[62]:\n\n\ndisplay_eval_df(response_vector, eval_result)\n\n\n# In[ ]:\n\n\nresponse_vector = vector_index.query(\"Who is the mayor of New York City?\")\neval_result = evaluator_gpt4.evaluate(response_vector)\n\n\n# In[64]:\n\n\ndisplay_eval_df(response_vector, eval_result)\n\n\n# In[ ]:\n\n\n\n\n", "examples/evaluation/GuardrailsDemo.py": "examples/evaluation/GuardrailsDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Simple Index Demo\n\n# #### Load documents, build the GPTSimpleVectorIndex\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\n# In[2]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# In[3]:\n\n\nindex = GPTSimpleVectorIndex.from_documents(documents, chunk_size_limit=512)\n\n\n# In[4]:\n\n\n# save index to disk\nindex.save_to_disk('index_simple.json')\n\n\n# In[3]:\n\n\n# load index from disk\nindex = GPTSimpleVectorIndex.load_from_disk('index_simple.json')\n\n\n# #### Define Query + Guardrails Spec\n\n# In[4]:\n\n\nfrom gpt_index.output_parsers import GuardrailsOutputParser\nfrom gpt_index.llm_predictor import StructuredLLMPredictor\n\n\n# In[5]:\n\n\nllm_predictor = StructuredLLMPredictor()\n\n\n# **Define custom QA and Refine Prompts**\n\n# In[7]:\n\n\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\n\n\n# In[16]:\n\n\n# NOTE: we don't need to define the query_str in the rail spec, we can define during query-time.\nrail_spec = (\"\"\"\n<rail version=\"0.1\">\n\n<output>\n    <list name=\"points\" description=\"Bullet points regarding events in the author's life.\">\n        <object>\n            <string name=\"explanation\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation2\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation3\" format=\"one-line\" on-fail-one-line=\"noop\" />\n        </object>\n    </list>\n</output>\n\n<prompt>\n\nQuery string here.\n\n@xml_prefix_prompt\n\n{output_schema}\n\n@json_suffix_prompt_v2_wo_none\n</prompt>\n</rail>\n\"\"\")\n\n\n# In[17]:\n\n\noutput_parser = GuardrailsOutputParser.from_rail_string(rail_spec, llm=llm_predictor.llm)\n\n\n# In[18]:\n\n\n# NOTE: we use the same output parser for both prompts, though you can choose to use different parsers\n# NOTE: here we add formatting instructions to the prompts.\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\n\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\n\n# In[19]:\n\n\n# take a look at the new QA template! \nprint(fmt_qa_tmpl)\n\n\n# #### Query Index\n\n# In[11]:\n\n\nresponse = index.query(\n    \"What are the three items the author did growing up?\", \n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n    llm_predictor=llm_predictor\n)\n\n\n# In[13]:\n\n\nprint(response)\n\n", "examples/chatgpt_plugin/ChatGPTRetrievalPluginReaderDemo.py": "examples/chatgpt_plugin/ChatGPTRetrievalPluginReaderDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # ChatGPT Retrieval Plugin Reader Demo\n# \n# Use our reader plugin to load data from ChatGPT\n\n# In[1]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# #### Load documents\n\n# In[ ]:\n\n\nfrom gpt_index.readers import ChatGPTRetrievalPluginReader\nimport os\n\n\n# In[4]:\n\n\nbearer_token = os.getenv(\"BEARER_TOKEN\")\n\n\n# In[6]:\n\n\n# load documents\nreader = ChatGPTRetrievalPluginReader(\n    endpoint_url=\"http://localhost:8000\",\n    bearer_token=bearer_token\n)\n\ndocuments = reader.load_data(\"What did the author do growing up?\")\n\n\n# In[12]:\n\n\nlen(documents)\n\n\n# #### Build Index\n\n# In[17]:\n\n\nfrom gpt_index import GPTListIndex\n\n\n# In[18]:\n\n\nindex = GPTListIndex(documents)\n\n\n# #### Query Index\n\n# In[19]:\n\n\nfrom IPython.display import Markdown, display\n\n\n# In[20]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\n    \"Summarize the retrieved content and describe what the author did growing up\",\n    response_mode=\"compact\"\n) \n\n\n# In[21]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n\n\n", "examples/chatgpt_plugin/ChatGPTRetrievalPluginIndexDemo.py": "examples/chatgpt_plugin/ChatGPTRetrievalPluginIndexDemo.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # ChatGPT Retrieval Plugin Index Demo\n\n# In[2]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# #### Load documents, build index\n\n# In[3]:\n\n\nfrom gpt_index.indices.vector_store import ChatGPTRetrievalPluginIndex\nfrom gpt_index import SimpleDirectoryReader\nimport os\n\n\n# In[4]:\n\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n\n# Get bearer token.\n# \n# Try following [this tutorial](https://www.ibm.com/docs/da/order-management?topic=SSGTJF/configuration/t_GeneratingJWTToken.htm) to generate a JWT token.\n\n# In[5]:\n\n\nbearer_token = os.getenv(\"BEARER_TOKEN\")\n\n\n# In[6]:\n\n\n# initialize without metadata filter\nindex = ChatGPTRetrievalPluginIndex.from_documents(\n    documents, \n    endpoint_url=\"http://localhost:8000\",\n    bearer_token=bearer_token,\n)\n\n\n# #### Query Index\n\n# In[7]:\n\n\nfrom IPython.display import Markdown, display\n\n\n# In[8]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = index.query(\"What did the author do growing up?\", similarity_top_k=3)\n\n\n# In[9]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n\n\n", "examples/chatgpt_plugin/ChatGPT_Retrieval_Plugin_Upload.py": "examples/chatgpt_plugin/ChatGPT_Retrieval_Plugin_Upload.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# This tutorial walks through how to use LlamaHub to load documents and upload these documents to a vector store through the ChatGPT Retrieval Plugin.\n\n# In[1]:\n\n\nget_ipython().system('git clone https://github.com/openai/chatgpt-retrieval-plugin.git')\n\n\n# In[ ]:\n\n\nget_ipython().system('pip install poetry')\n\n\n# #### Run these commands in a separate shell\n# \n# Please run the following commands in a separate terminal (we will be spinning up the local server):\n# \n# ```bash\n# cd chatgpt-retrieval-plugin && \\\n# poetry env use python3.10 && \\\n# poetry shell && \\\n# poetry install && \\\n# poetry run start\n# ```\n\n# ## Import/Convert LlamaIndex Documents\n\n# In[25]:\n\n\nfrom llama_index import download_loader, Document\nfrom typing import Dict, List\nimport json\n\n\n# In[26]:\n\n\nSimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n\nloader = SimpleWebPageReader(html_to_text=True)\nurl = \"http://www.paulgraham.com/worked.html\"\ndocuments = loader.load_data(urls=[url])\n\n\n# In[20]:\n\n\n# Convert LlamaIndex Documents to JSON format\n\ndef dump_docs_to_json(documents: List[Document], out_path: str) -> Dict:\n    \"\"\"Convert LlamaIndex Documents to JSON format and save it.\"\"\"\n    result_json = []\n    for doc in documents:\n        cur_dict = {\n            \"text\": doc.get_text(),\n            \"id\": doc.get_doc_id(),\n            # NOTE: feel free to customize the other fields as you wish\n            # fields taken from https://github.com/openai/chatgpt-retrieval-plugin/tree/main/scripts/process_json#usage\n            # \"source\": ...,\n            # \"source_id\": ...,\n            # \"url\": url,\n            # \"created_at\": ...,\n            # \"author\": \"Paul Graham\",\n        }\n        result_json.append(cur_dict)\n    \n    json.dump(result_json, open(out_path, 'w'))\n\n\n# In[21]:\n\n\ndump_docs_to_json(documents, \"docs.json\")\n\n\n# ## Upload to Docstore server\n# \n# We now follow the instructions (taken from https://github.com/openai/chatgpt-retrieval-plugin/tree/main/scripts/process_json) to upload your JSON documents to your docstore.\n# \n# You need to set environment variables indicating 1) the docstore you're using, and 2) how to authenticate into the docstore.\n# \n# Please follow these instructions https://github.com/openai/chatgpt-retrieval-plugin#development.\n\n# ### Pinecone example\n# \n# Below, we give an example for connecting to Pinecone.\n# \n# We first define the bearer token.\n# (Note: If you were confused like me as to how to generate a bearer token, use jwt.io and follow [this tutorial](https://www.ibm.com/docs/da/order-management?topic=SSGTJF/configuration/t_GeneratingJWTToken.htm)\n# \n# ```bash\n# export BEARER_TOKEN=<bearer_token>\n# ```\n# \n# Now define the rest of the environment variables.\n# \n# ```bash\n# export DATASTORE=<datastore>\n# export PINECONE_API_KEY=<pinecone_api_key>\n# export PINECONE_ENVIRONMENT=<pinecone_environment>\n# export PINECONE_INDEX=<pinecone_index>\n# ```\n\n# #### Run the `process_json.py` script\n# \n# The process_json.py script will take our document, chunk it up under the hood, and upload it to the docstore.\n# \n# **NOTE**: we disable `--screen_for_pii` and `--extract_metadata`. Toggling these flags will call the OpenAI chat completion endpoint,\n# leading to potential context size too big errors.\n# \n# Copy and paste the following command into a terminal\n# ```python\n# cd scripts/process_json && \\\n# python process_json.py --filepath path/to/docs.json --custom_metadata '{\"source\": \"file\"}'\n# ```\n\n# **NOTE**: If the command above returns `ModuleNotFoundError: No module named 'models`, try `pip install -e .` from the root directory of `chatgpt-retrieval-plugin` and rerun.\n\n# This should now upload documents to your vector db of choice! \n\n# In[ ]:\n\n\n\n\n", "examples/test_wiki/TestNYC_Embeddings.py": "examples/test_wiki/TestNYC_Embeddings.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # NYC Wikipedia Embeddings Demo\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# Demonstrate embedding capabilities in GPTTreeIndex and GPTListIndex\n\n# ### Setup + Data Prep\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\n# fetch \"New York City\" page from Wikipedia\nfrom pathlib import Path\n\nimport requests\nresponse = requests.get(\n    'https://en.wikipedia.org/w/api.php',\n    params={\n        'action': 'query',\n        'format': 'json',\n        'titles': 'New York City',\n        'prop': 'extracts',\n        # 'exintro': True,\n        'explaintext': True,\n    }\n).json()\npage = next(iter(response['query']['pages'].values()))\nnyc_text = page['extract']\n\ndata_path = Path('data')\nif not data_path.exists():\n    Path.mkdir(data_path)\n\nwith open('data/nyc_text.txt', 'w') as fp:\n    fp.write(nyc_text)\n\n\n# In[ ]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# ### GPTTreeIndex - Embedding-based Query\n\n# In[ ]:\n\n\nfrom llama_index import GPTTreeIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown\n\n\n# In[ ]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTTreeIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\nindex.save_to_disk('index.json')\n\n\n# In[ ]:\n\n\nnew_index = GPTTreeIndex.load_from_disk('index.json')\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = new_index.query(\"What is the name of the professional women's basketball team in New York City?\", mode=\"embedding\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\nresponse = new_index.query(\n    \"What battles took place in New York City in the American Revolution?\", \n    mode=\"embedding\"\n)\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = new_index.query(\"What are the airports in New York City?\", mode=\"embedding\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# ### GPTListIndex - Embedding-based Query\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown\n\n\n# In[ ]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTListIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\nindex.save_to_disk('index_list_emb.json')\n\n\n# In[ ]:\n\n\n# try loading\nnew_index = GPTListIndex.load_from_disk('index_list_emb.json')\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = new_index.query(\"What is the name of the professional women's basketball team in New York City?\", mode=\"embedding\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = new_index.query(\"What battles took place in New York City in the American Revolution?\", mode=\"embedding\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\nresponse = new_index.query(\"What are the airports in New York City?\", mode=\"embedding\")\n\n\n# In[ ]:\n\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n\n# ## Try out other embeddings! \n# (courtesy of langchain)\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, SimpleDirectoryReader, ServiceContext\nfrom IPython.display import Markdown\n\n\n# In[ ]:\n\n\n# load in HF embedding model from langchain\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom llama_index import LangchainEmbedding\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n\n\n# In[ ]:\n\n\n# try loading index\nnew_index = GPTListIndex.load_from_disk('index_list_emb.json')\n\n\n# In[ ]:\n\n\n# configure\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n\n# set Logging to DEBUG for more detailed outputs\nresponse = new_index.query(\n    \"What is the name of the professional women's basketball team in New York City?\", \n    mode=\"embedding\", \n    service_context=service_context, \n)\n\n\n# In[ ]:\n\n\nresponse\n\n\n# In[ ]:\n\n\n\n\n", "examples/test_wiki/TestNYC.py": "examples/test_wiki/TestNYC.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\n# fetch \"New York City\" page from Wikipedia\nfrom pathlib import Path\n\nimport requests\nresponse = requests.get(\n    'https://en.wikipedia.org/w/api.php',\n    params={\n        'action': 'query',\n        'format': 'json',\n        'titles': 'New York City',\n        'prop': 'extracts',\n        # 'exintro': True,\n        'explaintext': True,\n    }\n).json()\npage = next(iter(response['query']['pages'].values()))\nnyc_text = page['extract']\n\ndata_path = Path('data')\nif not data_path.exists():\n    Path.mkdir(data_path)\n\nwith open('data/nyc_text.txt', 'w') as fp:\n    fp.write(nyc_text)\n\n\n# In[ ]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# In[ ]:\n\n\nfrom llama_index import GPTTreeIndex, SimpleDirectoryReader\n\n\n# In[ ]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTTreeIndex.from_documents(documents)\n\n\n# In[ ]:\n\n\nindex.save_to_disk('index.json')\n\n\n# In[ ]:\n\n\n# try loading\nnew_index = GPTTreeIndex.load_from_disk('index.json')\n\n\n# In[ ]:\n\n\n# GPT doesn't find the corresponding evidence in the leaf node, but still gives the correct answer\n# set Logging to DEBUG for more detailed outputs\n\nnew_index.query(\"What is the name of the professional women's basketball team in New York City?\")\n\n\n# In[ ]:\n\n\n# GPT doesn't find the corresponding evidence in the leaf node, but still gives the correct answer\n# set Logging to DEBUG for more detailed outputs\n\nnew_index.query(\"What battles took place in New York City in the American Revolution?\")\n\n\n# In[ ]:\n\n\n# GPT doesn't find the corresponding evidence in the leaf node, but still gives the correct answer\n# set Logging to DEBUG for more detailed outputs\n\nnew_index.query(\"What are the airports in New York City?\")\n\n\n# In[ ]:\n\n\n# Try using embedding query\nnew_index.query(\"What are the airports in New York City?\", mode=\"embedding\")\n\n", "examples/test_wiki/TestWikiReader.py": "examples/test_wiki/TestWikiReader.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n\n\n# ## Wikipedia Reader + Keyword Table\n\n# In[36]:\n\n\nfrom llama_index import GPTKeywordTableIndex, WikipediaReader\n\n\n# In[37]:\n\n\nwiki_docs = WikipediaReader().load_data(pages=['Covid-19'])\n\n\n# In[ ]:\n\n\nindex = GPTKeywordTableIndex.from_documents(wiki_docs)\n\n\n# In[42]:\n\n\n# save index to docs\nindex.save_to_disk('index_covid.json')\n\n\n# In[45]:\n\n\nnew_index = GPTKeywordTableIndex.load_from_disk('index_covid.json')\n\n\n# In[46]:\n\n\n# GPT doesn't find the corresponding evidence in the leaf node, but still gives the correct answer\n# set Logging to DEBUG for more detailed outputs\nnew_index.query(\"Which country included tocilizumab in treatment for covid-19?\")\n\n\n# ## Wikipedia Reader + List\n\n# In[ ]:\n\n\nfrom llama_index import GPTListIndex, WikipediaReader\n\n\n# In[3]:\n\n\nwiki_docs = WikipediaReader().load_data(pages=['Covid-19'])\n\n\n# In[4]:\n\n\nindex = GPTListIndex.from_documents(wiki_docs)\n\n\n# In[13]:\n\n\n# set Logging to DEBUG for more detailed outputs\n# with keyword lookup\nresponse = index.query(\n    \"Which country included tocilizumab in treatment for covid-19?\", \n    required_keywords=[\"tocilizumab\"]\n)\n\n\n# In[18]:\n\n\ndisplay(response.strip())\n\n\n# In[19]:\n\n\n# set Logging to DEBUG for more detailed outputs\n# without keyword lookup\nresponse = index.query(\n    \"Which country included tocilizumab in treatment for covid-19?\"\n)\n\n\n# In[20]:\n\n\ndisplay(response.strip())\n\n", "examples/test_wiki/TestNYC-Tree-GPT4.py": "examples/test_wiki/TestNYC-Tree-GPT4.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[18]:\n\n\nimport logging, sys\n# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n# Uncomment if you want to temporarily disable logger\nlogging.disable(sys.maxsize)\n\n\n# In[19]:\n\n\n# fetch \"New York City\" page from Wikipedia\nfrom pathlib import Path\n\nimport requests\nresponse = requests.get(\n    'https://en.wikipedia.org/w/api.php',\n    params={\n        'action': 'query',\n        'format': 'json',\n        'titles': 'New York City',\n        'prop': 'extracts',\n        # 'exintro': True,\n        'explaintext': True,\n    }\n).json()\npage = next(iter(response['query']['pages'].values()))\nnyc_text = page['extract']\n\ndata_path = Path('data')\nif not data_path.exists():\n    Path.mkdir(data_path)\n\nwith open('data/nyc_text.txt', 'w') as fp:\n    fp.write(nyc_text)\n\n\n# In[3]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"\"\n\n\n# In[2]:\n\n\nfrom llama_index import GPTTreeIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\nfrom llama_index.logger import LlamaLogger\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n\n\n# In[3]:\n\n\n# gpt-3 (davinci)\nllm_predictor_gpt3 = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\nservice_context_gpt3 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt3)\n\n# gpt-4\nllm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context_gpt4 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt4)\n\n\n# In[5]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\n\n# In[ ]:\n\n\nindex = GPTTreeIndex.from_documents(documents, service_context=service_context_gpt4)\n\n\n# In[17]:\n\n\nindex.save_to_disk('index_gpt4.json')\n\n\n# In[4]:\n\n\n# try loading\nnew_index = GPTTreeIndex.load_from_disk('index_gpt4.json')\n\n\n# In[5]:\n\n\nresponse_gpt4 = new_index.query(\n    \"What battles took place in New York City in the American Revolution?\",\n    service_context=service_context_gpt4,\n    verbose=True\n)\n\n\n# In[10]:\n\n\nstr(response_gpt4)\n\n\n# In[8]:\n\n\nresponse_gpt4.source_nodes[0]\n\n\n# In[30]:\n\n\nresponse_gpt3 = new_index.query(\n    \"What battles took place in New York City in the American Revolution?\",\n    service_context=service_context_gpt3,\n    verbose=True\n)\n\n\n# In[12]:\n\n\nstr(response_gpt3)\n\n\n# In[13]:\n\n\nresponse_gpt3.source_nodes[0]\n\n\n# In[31]:\n\n\nresponse_gpt4 = new_index.query(\n    \"What are the airports in New York City?\",\n    service_context=service_context_gpt4,\n    verbose=True\n)\n\n\n# In[15]:\n\n\nstr(response_gpt4)\n\n\n# In[17]:\n\n\nresponse_gpt4.source_nodes[0].source_text\n\n\n# In[32]:\n\n\nresponse_gpt3 = new_index.query(\n    \"What are the airports in New York City?\",\n    service_context=service_context_gpt3,\n    verbose=True\n)\n\n\n# In[33]:\n\n\nprint(str(response_gpt3))\n\n\n# In[21]:\n\n\nresponse_gpt3.source_nodes[0].source_text\n\n\n# In[ ]:\n\n\nresponse_gpt4 = new_index.query(\n    \"What battles took place in New York City in the American Revolution?\",\n    service_context=service_context_gpt4,\n    verbose=True\n)\n\n\n# In[ ]:\n\n\nprint(str(response_gpt4))\n\n\n# In[ ]:\n\n\nresponse_gpt4.source_nodes[0].source_text\n\n\n# In[28]:\n\n\nresponse_gpt4 = new_index.query(\n    \"Who is the current mayor of New York City?\",\n    service_context=service_context_gpt4,\n    verbose=True\n)\n\n\n# In[29]:\n\n\nprint(str(response_gpt4))\n\n\n# In[24]:\n\n\nresponse_gpt4.source_nodes[0].source_text\n\n\n# In[8]:\n\n\nresponse_gpt3 = new_index.query(\n    \"Who is the current mayor of New York City?\",\n    service_context=service_context_gpt3,\n    verbose=True\n)\n\n\n# In[9]:\n\n\nprint(str(response_gpt3))\n\n\n# In[10]:\n\n\nresponse_gpt3.source_nodes[0].source_text\n\n\n# In[12]:\n\n\nlogger = LlamaLogger()\nresponse_gpt4 = new_index.query(\n    \"What is the demographic breakdown of NYC by ethnicity?\",\n    service_context=service_context_gpt4,\n    verbose=True,\n    llama_logger=logger\n)\n\n\n# In[14]:\n\n\nstr(response_gpt4)\n\n\n# In[16]:\n\n\nresponse_gpt3 = new_index.query(\n    \"What is the demographic breakdown of NYC by ethnicity?\",\n    service_context=service_context_gpt3,\n    verbose=True,\n    llama_logger=logger\n)\n\n\n# In[18]:\n\n\nstr(response_gpt3)\n\n\n# In[19]:\n\n\nlogger = LlamaLogger()\nresponse_gpt4 = new_index.query(\n    \"Why is congestion pricing in NYC being introduced?\",\n    service_context=service_context_gpt4,\n    verbose=True,\n    llama_logger=logger\n)\n\n\n# In[23]:\n\n\nstr(response_gpt4)\n\n\n# In[27]:\n\n\nstr(response_gpt4.source_nodes[0].source_text)\n\n\n# In[21]:\n\n\nresponse_gpt3 = new_index.query(\n    \"Why is congestion pricing in NYC being introduced?\",\n    service_context=service_context_gpt3,\n    verbose=True,\n    llama_logger=logger\n)\n\n\n# In[22]:\n\n\nstr(response_gpt3)\n\n\n# In[ ]:\n\n\n\n\n", "examples/test_wiki/TestNYC-Benchmark-GPT4.py": "examples/test_wiki/TestNYC-Benchmark-GPT4.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[17]:\n\n\nimport logging, sys\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n# Uncomment if you want to temporarily disable logger\nlogging.disable(sys.maxsize)\n\n\n# In[2]:\n\n\n# NOTE: only necessary for querying with `use_async=True` in notebook\nimport nest_asyncio\nnest_asyncio.apply()\n\n\n# In[3]:\n\n\n# My OpenAI Key\nimport os\nos.environ['OPENAI_API_KEY'] = \"\"\n\n\n# In[3]:\n\n\nfrom gpt_index import GPTTreeIndex, SimpleDirectoryReader, LLMPredictor, GPTSimpleVectorIndex, GPTListIndex, Prompt, ServiceContext\nfrom gpt_index.indices.base import BaseGPTIndex\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\nfrom gpt_index.response.schema import Response\nimport pandas as pd\nfrom typing import Tuple\n\n\n# # Setup data\n\n# In[2]:\n\n\n# fetch \"New York City\" page from Wikipedia\nfrom pathlib import Path\n\nimport requests\nresponse = requests.get(\n    'https://en.wikipedia.org/w/api.php',\n    params={\n        'action': 'query',\n        'format': 'json',\n        'titles': 'New York City',\n        'prop': 'extracts',\n        # 'exintro': True,\n        'explaintext': True,\n    }\n).json()\npage = next(iter(response['query']['pages'].values()))\nnyc_text = page['extract']\n\ndata_path = Path('data')\nif not data_path.exists():\n    Path.mkdir(data_path)\n\nwith open('data/nyc_text.txt', 'w') as fp:\n    fp.write(nyc_text)\n\n\n# In[4]:\n\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\n\n# # Setup benchmark\n\n# In[5]:\n\n\nfrom dataclasses import dataclass\nfrom typing import List\n\n\n# In[6]:\n\n\n@dataclass\nclass TestCase:\n    query: str \n    must_contain: List[str]\n\n\n# In[7]:\n\n\n@dataclass\nclass TestOutcome:\n    test: TestCase\n    response: Response\n    \n    @property\n    def is_correct_response(self) -> bool:\n        is_correct = True\n        for answer in self.test.must_contain:\n            if answer not in self.response.response:\n                is_correct = False\n        return is_correct\n    \n    @property\n    def is_correct_source(self) -> bool:\n        is_correct = True\n        for answer in self.test.must_contain:\n            if all(answer not in node.source_text for node in self.response.source_nodes):\n                is_correct = False\n        return is_correct\n\n\n# In[8]:\n\n\nclass Benchmark:\n    def __init__(self, tests: List[TestCase]) -> None:\n        self._tests = tests\n    \n    def test(self, index: BaseGPTIndex, llm_predictor: LLMPredictor, **kwargs) -> List[TestOutcome]:\n        outcomes: List[TestOutcome] = []\n        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n        for test in self._tests:\n            response = index.query(\n                test.query,\n                service_context=service_context,\n                **kwargs\n            )\n            outcome = TestOutcome(test=test, response=response)\n            outcomes.append(outcome)\n        return outcomes\n\n\n# In[9]:\n\n\ndef analyze_outcome(outcomes: List[TestOutcome]) -> None:\n    rows = []\n    for outcome in outcomes:\n        row = [outcome.test.query, outcome.is_correct_response, outcome.is_correct_source]\n        rows.append(row)\n    df = pd.DataFrame(rows, columns=['Test Query', 'Correct Response', 'Correct Source'])\n    return df\n\n\n# In[10]:\n\n\ntest_battle = TestCase(\n    query=\"What battles took place in New York City in the American Revolution?\",\n    must_contain=[\"Battle of Long Island\"]\n)\n\ntest_mayor = TestCase(\n    query='Who was elected as the mayor after the Great Depression?',\n    must_contain=[\"Fiorello La Guardia\"]\n)\n\ntest_tourists = TestCase(\n    query='How many tourists visited New York City in 2019?',\n    must_contain=['66.6 million']\n)\ntest_airport = TestCase(\n    query='What are the airports in New York City?',\n    must_contain=['LaGuardia Airport']\n)\ntest_visit = TestCase(\n    query='When was the first documented visit into New York Harbor?',\n    must_contain=['1524']\n)\n\n\n# In[11]:\n\n\nbm = Benchmark([\n    test_battle,\n    test_mayor,\n    test_tourists,\n    test_airport,\n    test_visit,\n])\n\n\n# # LLM based evaluation\n\n# In[592]:\n\n\nfrom gpt_index.prompts.prompt_type import PromptType\n\nEVAL_PROMPT_TMPL = (\n    \"Given the question below. \\n\"\n    \"---------------------\\n\"\n    \"{query_str}\"\n    \"\\n---------------------\\n\"\n    \"Decide if the following retreived context is relevant. \\n\"\n    \"\\n---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Then decide if the answer is correct. \\n\"\n    \"\\n---------------------\\n\"\n    \"{answer_str}\"\n    \"\\n---------------------\\n\"\n    \"Answer in the following format:\\n\"\n    \"'Context is relevant: <True>\\nAnswer is correct: <True>' \"\n    \"and explain why.\"\n)\n\nclass EvalPrompt(Prompt):\n    prompt_type: PromptType = PromptType.CUSTOM\n    input_variables: List[str] = [\"query_str\", 'context_str', 'answer_str']\n\nDEFAULT_EVAL_PROMPT = EvalPrompt(EVAL_PROMPT_TMPL)\n\n\n# In[593]:\n\n\nimport re\ndef extract_eval_result(result_str: str):\n    boolean_pattern = r\"(True|False)\"\n    matches = re.findall(boolean_pattern, result_str)\n    return [match == \"True\" for match in matches]    \n\n\n# In[594]:\n\n\ndef analyze_outcome_llm_single(outcome: TestOutcome, llm_predictor: LLMPredictor) -> Tuple[bool, bool]:\n    try:\n        source_text = outcome.response.source_nodes[0].source_text\n    except:\n        source_text = \"Failed to retrieve any context\"\n    result_str, _ = llm_predictor.predict(\n        DEFAULT_EVAL_PROMPT,\n        query_str=outcome.test.query,\n        context_str=source_text,\n        answer_str=outcome.response.response\n    )\n    is_context_relevant, is_answer_correct = extract_eval_result(result_str)\n    return is_answer_correct, is_context_relevant, result_str\n\ndef analyze_outcome_llm(outcomes: List[TestOutcome], llm_predictor: LLMPredictor) -> None:\n    rows = []\n    for outcome in outcomes:\n        is_correct_response, is_correct_source, result_str = analyze_outcome_llm_single(outcome, llm_predictor)\n        row = [outcome.test.query, is_correct_response, is_correct_source, result_str]\n        rows.append(row)\n    df = pd.DataFrame(rows, columns=['Test Query', 'Correct Response (LLM)', 'Correct Source (LLM)', 'Eval (LLM)'])\n    return df\n\n\n# # Build Indices\n\n# In[643]:\n\n\nvector_index = GPTSimpleVectorIndex.from_documents(\n    documents, \n)\n\n\n# In[473]:\n\n\nlist_index = GPTListIndex.from_documents(\n    documents, \n)\n\n\n# In[468]:\n\n\ntree_index = GPTTreeIndex.from_documents(documents)\n\n\n# In[632]:\n\n\n# Save indices\nvector_index.save_to_disk('vector_index.json')\ntree_index.save_to_disk('tree_index.json')\nlist_index.save_to_disk('list_index.json')\n\n\n# In[13]:\n\n\n# Load indices\ntree_index = GPTTreeIndex.load_from_disk('tree_index.json')\nlist_index = GPTListIndex.load_from_disk('list_index.json')\nvector_index = GPTSimpleVectorIndex.load_from_disk('vector_index.json')\n\n\n# # Create LLMPredictors\n\n# In[12]:\n\n\n# gpt-4\nllm_predictor_gpt4 = LLMPredictor(\n    llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n)\n\n\n# In[169]:\n\n\n# gpt-3 (text-davinci-003)\nllm_predictor_gpt3 = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\n\n\n# In[22]:\n\n\n# chatgpt (gpt-3.5-turbo)\nllm_predictor_chatgpt = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n\n\n# # Benchmarking \n\n# ### Tree Index + GPT4\n\n# In[583]:\n\n\noutcomes_tree_gpt4 = bm.test(tree_index, llm_predictor_gpt4)\n\n\n# In[584]:\n\n\nanalyze_outcome(outcomes_tree_gpt4)\n\n\n# ### Tree Index + GPT3\n\n# In[549]:\n\n\noutcomes_tree_gpt3 = bm.test(tree_index, llm_predictor_gpt3)\n\n\n# In[550]:\n\n\nanalyze_outcome(outcomes_tree_gpt3)\n\n\n# ### List Index + GPT4\n\n# In[18]:\n\n\noutcomes_list_gpt4 = bm.test(list_index, llm_predictor_gpt4, response_mode=\"tree_summarize\", use_async=True)\n\n\n# In[19]:\n\n\nanalyze_outcome(outcomes_list_gpt4)\n\n\n# ### List Index + GPT3\n\n# In[501]:\n\n\noutcomes_list_gpt3 = bm.test(list_index, llm_predictor_gpt3, response_mode=\"tree_summarize\", use_async=True)\n\n\n# In[502]:\n\n\nanalyze_outcome(outcomes_list_gpt3)\n\n\n# ### List Index + ChatGPT\n\n# In[23]:\n\n\noutcomes_list_chatgpt = bm.test(list_index, llm_predictor_chatgpt, response_mode=\"tree_summarize\", use_async=True)\n\n\n# In[24]:\n\n\nanalyze_outcome(outcomes_list_chatgpt)\n\n\n# ### Vector Store Index + GPT4 \n\n# In[487]:\n\n\noutcomes_vector_gpt4 = bm.test(vector_index, llm_predictor_gpt4)\n\n\n# In[488]:\n\n\nanalyze_outcome(outcomes_vector_gpt4)\n\n\n# ### Vector Store Index + GPT3\n\n# In[644]:\n\n\noutcomes_vector_gpt3 = bm.test(vector_index, llm_predictor_gpt3)\n\n\n# In[645]:\n\n\nanalyze_outcome(outcomes_vector_gpt3)\n\n\n# # LLM based Evaluation\n\n# In[646]:\n\n\nanalyze_outcome(outcomes_vector_gpt3)\n\n\n# In[647]:\n\n\neval_gpt4 = analyze_outcome_llm(outcomes_vector_gpt3, llm_predictor_gpt4)\n\n\n# In[657]:\n\n\neval_gpt4\n\n\n# In[651]:\n\n\neval_chatgpt = analyze_outcome_llm(outcomes_vector_gpt3, llm_predictor_chatgpt)\n\n\n# In[652]:\n\n\neval_chatgpt\n\n\n# In[649]:\n\n\neval_gpt3 = analyze_outcome_llm(outcomes_vector_gpt3, llm_predictor_gpt3)\n\n\n# In[650]:\n\n\neval_gpt3\n\n", "examples/composable_indices/ComposableIndices.py": "examples/composable_indices/ComposableIndices.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Composable Indices Demo\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom llama_index import (\n    GPTSimpleVectorIndex, \n    GPTSimpleKeywordTableIndex, \n    GPTListIndex, \n    SimpleDirectoryReader\n)\n\n\n# #### Load Datasets\n# \n# Load both the NYC Wikipedia page as well as Paul Graham's \"What I Worked On\" essay\n\n# In[ ]:\n\n\n# fetch \"New York City\" page from Wikipedia\nfrom pathlib import Path\n\nimport requests\nresponse = requests.get(\n    'https://en.wikipedia.org/w/api.php',\n    params={\n        'action': 'query',\n        'format': 'json',\n        'titles': 'New York City',\n        'prop': 'extracts',\n        # 'exintro': True,\n        'explaintext': True,\n    }\n).json()\npage = next(iter(response['query']['pages'].values()))\nnyc_text = page['extract']\n\ndata_path = Path('data')\nif not data_path.exists():\n    Path.mkdir(data_path)\n\nwith open('../test_wiki/data/nyc_text.txt', 'w') as fp:\n    fp.write(nyc_text)\n\n\n# In[ ]:\n\n\n# load NYC dataset\nnyc_documents = SimpleDirectoryReader('../test_wiki/data/').load_data()\n\n\n# In[ ]:\n\n\n# load PG's essay\nessay_documents = SimpleDirectoryReader('../paul_graham_essay/data/').load_data()\n\n\n# ### Building the document indices\n# Build a tree index for the NYC wiki page and PG essay\n\n# In[ ]:\n\n\n# build NYC index\nnyc_index = GPTSimpleVectorIndex.from_documents(nyc_documents)\n\n\n# In[ ]:\n\n\nnyc_index.save_to_disk('index_nyc.json')\n\n\n# In[ ]:\n\n\n# build essay index\nessay_index = GPTSimpleVectorIndex.from_documents(essay_documents)\n\n\n# In[ ]:\n\n\nessay_index.save_to_disk('index_pg.json')\n\n\n# ### Loading the indices\n# Build a tree indices for the NYC wiki page and PG essay\n\n# In[ ]:\n\n\n# try loading\nnyc_index = GPTSimpleVectorIndex.load_from_disk('index_nyc.json')\nessay_index = GPTSimpleVectorIndex.load_from_disk('index_pg.json')\n\n\n# ### Set summaries for the indices\n# \n# Add text summaries to indices, so we can compose other indices on top of it\n\n# In[ ]:\n\n\nnyc_index_summary = \"\"\"\n    New York, often called New York City or NYC, \n    is the most populous city in the United States. \n    With a 2020 population of 8,804,190 distributed over 300.46 square miles (778.2 km2), \n    New York City is also the most densely populated major city in the United States, \n    and is more than twice as populous as second-place Los Angeles. \n    New York City lies at the southern tip of New York State, and \n    constitutes the geographical and demographic center of both the \n    Northeast megalopolis and the New York metropolitan area, the \n    largest metropolitan area in the world by urban landmass.[8] With over \n    20.1 million people in its metropolitan statistical area and 23.5 million \n    in its combined statistical area as of 2020, New York is one of the world's \n    most populous megacities, and over 58 million people live within 250 mi (400 km) of \n    the city. New York City is a global cultural, financial, and media center with \n    a significant influence on commerce, health care and life sciences, entertainment, \n    research, technology, education, politics, tourism, dining, art, fashion, and sports. \n    Home to the headquarters of the United Nations, \n    New York is an important center for international diplomacy,\n    an established safe haven for global investors, and is sometimes described as the capital of the world.\n\"\"\"\nessay_index_summary = \"\"\"\n    Author: Paul Graham. \n    The author grew up painting and writing essays. \n    He wrote a book on Lisp and did freelance Lisp hacking work to support himself. \n    He also became the de facto studio assistant for Idelle Weber, an early photorealist painter. \n    He eventually had the idea to start a company to put art galleries online, but the idea was unsuccessful. \n    He then had the idea to write software to build online stores, which became the basis for his successful company, Viaweb. \n    After Viaweb was acquired by Yahoo!, the author returned to painting and started writing essays online. \n    He wrote a book of essays, Hackers & Painters, and worked on spam filters. \n    He also bought a building in Cambridge to use as an office. \n    He then had the idea to start Y Combinator, an investment firm that would \n    make a larger number of smaller investments and help founders remain as CEO. \n    He and his partner Jessica Livingston ran Y Combinator and funded a batch of startups twice a year. \n    He also continued to write essays, cook for groups of friends, and explore the concept of invented vs discovered in software. \n\"\"\"\n\n\n# ### Build Keyword Table Index on top of tree indices! \n# \n# We set summaries for each of the NYC and essay indices, and then compose a keyword index on top of it.\n\n# In[ ]:\n\n\nfrom llama_index.indices.composability import ComposableGraph\n\n\n# In[ ]:\n\n\n# set query config\nquery_configs = [\n    {\n        \"index_struct_type\": \"simple_dict\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"similarity_top_k\": 1\n        }\n    },\n    {\n        \"index_struct_type\": \"keyword_table\",\n        \"query_mode\": \"simple\",\n        \"query_kwargs\": {}\n    },\n]\n\n\n# In[ ]:\n\n\ngraph = ComposableGraph.from_indices(\n    GPTSimpleKeywordTableIndex,\n    [nyc_index, essay_index], \n    index_summaries=[nyc_index_summary, essay_index_summary],\n    max_keywords_per_chunk=50\n)\n\n\n# In[ ]:\n\n\n# [optional] save to disk\ngraph.save_to_disk(\"index_graph.json\")\n\n\n# In[ ]:\n\n\n# [optional] load from disk\ngraph = ComposableGraph.load_from_disk(\"index_graph.json\")\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\n# ask it a question about NYC \nresponse = graph.query(\n    \"What is the climate of New York City like? How cold is it during the winter?\", \n    query_configs=query_configs\n)\n\n\n# In[ ]:\n\n\nprint(str(response))\n\n\n# In[ ]:\n\n\n# Get source of response\nprint(response.get_formatted_sources())\n\n\n# In[ ]:\n\n\n# ask it a question about PG's essay\nresponse = graph.query(\n    \"What did the author do growing up, before his time at Y Combinator?\", \n    query_configs=query_configs\n)\n\n\n# In[ ]:\n\n\nprint(str(response))\n\n\n# In[ ]:\n\n\n# Get source of response\nprint(response.get_formatted_sources())\n\n\n# In[ ]:\n\n\n\n\n", "examples/composable_indices/ComposableIndices-Prior.py": "examples/composable_indices/ComposableIndices-Prior.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Composable Indices Demo\n\n# In[ ]:\n\n\n# NOTE: This is ONLY necessary in jupyter notebook.\n# Details: Jupyter runs an event-loop behind the scenes. \n#          This results in nested event-loops when we start an event-loop to make async queries.\n#          This is normally not allowed, we use nest_asyncio to allow it for convenience.  \nimport nest_asyncio\nnest_asyncio.apply()\n\n\n# In[ ]:\n\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\n# In[ ]:\n\n\nfrom llama_index import (\n    GPTSimpleVectorIndex,\n    GPTEmptyIndex,\n    GPTTreeIndex,\n    GPTListIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n)\n\n\n# ### Load Datasets\n# \n# Load PG's essay\n\n# In[ ]:\n\n\n# load PG's essay\nessay_documents = SimpleDirectoryReader('../paul_graham_essay/data/').load_data()\n\n\n# ### Building the document indices\n# - Build a vector index for PG's essay\n# - Also build an empty index (to store prior knowledge)\n\n# In[ ]:\n\n\n# configure\nservice_context = ServiceContext.from_defaults(chunk_size_limit=512)\n\n# build essay index\nessay_index = GPTSimpleVectorIndex.from_documents(essay_documents, service_context=service_context)\nempty_index = GPTEmptyIndex()\n\n\n# In[ ]:\n\n\nessay_index.save_to_disk('index_pg.json')\n\n\n# ### Loading the indices\n# Build a vector index for PG's essay, build empty index.\n\n# In[ ]:\n\n\n# try loading\nessay_index = GPTSimpleVectorIndex.load_from_disk('index_pg.json')\nempty_index = GPTEmptyIndex()\n\n\n# ### Query Indices\n# See the response of querying each index\n\n# In[ ]:\n\n\nresponse = essay_index.query(\n    \"Tell me about what Sam Altman did during his time in YC\",\n    similarity_top_k=3,\n    response_mode=\"tree_summarize\"\n)\n\n\n# In[ ]:\n\n\nprint(str(response))\n\n\n# In[ ]:\n\n\nresponse = empty_index.query(\n    \"Tell me about what Sam Altman did during his time in YC\",\n)\n\n\n# In[ ]:\n\n\nprint(str(response))\n\n\n# Define summary for each index.\n\n# In[ ]:\n\n\nessay_index_summary = \"This document describes Paul Graham's life, from early adulthood to the present day.\"\nempty_index_summary = \"This can be used for general knowledge purposes.\"\n\n\n# ### Define Graph (List Index as Parent Index)\n# \n# This allows us to synthesize responses both using a knowledge corpus as well as prior knowledge.\n\n# In[ ]:\n\n\nfrom llama_index.indices.composability import ComposableGraph\n\n\n# In[ ]:\n\n\n# set query config\nquery_configs = [\n    {\n        \"index_struct_type\": \"simple_dict\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"similarity_top_k\": 3,\n            \"response_mode\": \"tree_summarize\"\n        }\n    },\n]\n\n\n# In[ ]:\n\n\ngraph = ComposableGraph.from_indices(\n    GPTListIndex,\n    [essay_index, empty_index], \n    index_summaries=[essay_index_summary, empty_index_summary]\n)\n\n\n# In[ ]:\n\n\n# [optional] save to disk\ngraph.save_to_disk(\"index_graph.json\")\n\n\n# In[ ]:\n\n\n# [optional] load from disk\ngraph = ComposableGraph.load_from_disk(\"index_graph.json\")\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\n# ask it a question about Sam Altman\nresponse = graph.query(\n    \"Tell me about what Sam Altman did during his time in YC\", \n    query_configs=query_configs,\n)\n\n\n# In[ ]:\n\n\nprint(str(response))\n\n\n# In[ ]:\n\n\n# Get source of response\nprint(response.get_formatted_sources())\n\n\n# ### Define Graph (Tree Index as Parent Index)\n# \n# This allows us to \"route\" a query to either a knowledge-augmented index, or to the LLM itself.\n\n# In[ ]:\n\n\nfrom llama_index.indices.composability import ComposableGraph\n\n\n# In[ ]:\n\n\n# set query config\nquery_configs = [\n    {\n        \"index_struct_type\": \"simple_dict\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"similarity_top_k\": 3,\n            \"response_mode\": \"tree_summarize\"\n        }\n    },\n]\n\n\n# In[ ]:\n\n\ngraph2 = ComposableGraph.from_indices(\n    GPTTreeIndex,\n    [essay_index, empty_index],\n    index_summaries=[essay_index_summary, empty_index_summary]\n)\n\n\n# In[ ]:\n\n\n# [optional] save to disk\ngraph2.save_to_disk(\"index_graph2.json\")\n\n\n# In[ ]:\n\n\n# [optional] load from disk\ngraph2 = ComposableGraph.load_from_disk(\"index_graph2.json\")\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\n# ask it a question about NYC \nresponse = graph2.query(\n    \"Tell me about what Paul Graham did growing up?\", \n    query_configs=query_configs\n)\n\n\n# In[ ]:\n\n\nstr(response)\n\n\n# In[ ]:\n\n\nprint(response.get_formatted_sources())\n\n\n# In[ ]:\n\n\nresponse = graph2.query(\n    \"Tell me about Barack Obama\", \n    query_configs=query_configs\n)\n\n\n# In[ ]:\n\n\nstr(response)\n\n\n# In[ ]:\n\n\nresponse.get_formatted_sources()\n\n\n# In[ ]:\n\n\n\n\n", "examples/composable_indices/ComposableIndices-Weaviate.py": "examples/composable_indices/ComposableIndices-Weaviate.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Composable Indices Demo\n\n# In[ ]:\n\n\nimport logging\nimport sys\nimport weaviate\nfrom pprint import pprint\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom gpt_index import (\n    GPTSimpleVectorIndex, \n    GPTSimpleKeywordTableIndex, \n    GPTListIndex, \n    GPTWeaviateIndex,\n    SimpleDirectoryReader\n)\n\n\n# In[ ]:\n\n\nresource_owner_config = weaviate.AuthClientPassword(\n  username = \"<username>\", \n  password = \"<password>\", \n)\n\n\n# In[ ]:\n\n\nclient = weaviate.Client(\"https://test-weaviate-cluster.semi.network/\", auth_client_secret=resource_owner_config)\n\n\n# In[ ]:\n\n\n# [optional] set batch\nclient.batch.configure(batch_size=10)\n\n\n# #### Load Datasets\n# \n# Load both the NYC Wikipedia page as well as Paul Graham's \"What I Worked On\" essay\n\n# In[ ]:\n\n\n# fetch \"New York City\" page from Wikipedia\nfrom pathlib import Path\n\nimport requests\nresponse = requests.get(\n    'https://en.wikipedia.org/w/api.php',\n    params={\n        'action': 'query',\n        'format': 'json',\n        'titles': 'New York City',\n        'prop': 'extracts',\n        # 'exintro': True,\n        'explaintext': True,\n    }\n).json()\npage = next(iter(response['query']['pages'].values()))\nnyc_text = page['extract']\n\ndata_path = Path('data')\nif not data_path.exists():\n    Path.mkdir(data_path)\n\nwith open('../test_wiki/data/nyc_text.txt', 'w') as fp:\n    fp.write(nyc_text)\n\n\n# In[ ]:\n\n\n# load NYC dataset\nnyc_documents = SimpleDirectoryReader('../test_wiki/data/').load_data()\n\n\n# In[ ]:\n\n\n# load PG's essay\nessay_documents = SimpleDirectoryReader('../paul_graham_essay/data/').load_data()\n\n\n# ### Building the document indices\n# Build a tree index for the NYC wiki page and PG essay\n\n# In[ ]:\n\n\n# build NYC index\nnyc_index = GPTWeaviateIndex.from_documents(nyc_documents, weaviate_client=client, class_prefix=\"Nyc_docs\")\n\n\n# In[ ]:\n\n\nnyc_index.save_to_disk('index_nyc.json')\n\n\n# In[ ]:\n\n\n# build essay index\nessay_index = GPTWeaviateIndex.from_documents(essay_documents, weaviate_client=client, class_prefix=\"Essay_docs\")\n\n\n# In[ ]:\n\n\nessay_index.save_to_disk('index_pg.json')\n\n\n# ### Loading the indices\n# Build a tree indices for the NYC wiki page and PG essay\n\n# In[ ]:\n\n\n# try loading\nnyc_index = GPTWeaviateIndex.load_from_disk('index_nyc.json', weaviate_client=client)\nessay_index = GPTWeaviateIndex.load_from_disk('index_pg.json', weaviate_client=client)\n\n\n# ### Set summaries for the indices\n# \n# Add text summaries to indices, so we can compose other indices on top of it\n\n# In[ ]:\n\n\nnyc_index_summary = \"\"\"\n    New York, often called New York City or NYC, \n    is the most populous city in the United States. \n    With a 2020 population of 8,804,190 distributed over 300.46 square miles (778.2 km2), \n    New York City is also the most densely populated major city in the United States, \n    and is more than twice as populous as second-place Los Angeles. \n    New York City lies at the southern tip of New York State, and \n    constitutes the geographical and demographic center of both the \n    Northeast megalopolis and the New York metropolitan area, the \n    largest metropolitan area in the world by urban landmass.[8] With over \n    20.1 million people in its metropolitan statistical area and 23.5 million \n    in its combined statistical area as of 2020, New York is one of the world's \n    most populous megacities, and over 58 million people live within 250 mi (400 km) of \n    the city. New York City is a global cultural, financial, and media center with \n    a significant influence on commerce, health care and life sciences, entertainment, \n    research, technology, education, politics, tourism, dining, art, fashion, and sports. \n    Home to the headquarters of the United Nations, \n    New York is an important center for international diplomacy,\n    an established safe haven for global investors, and is sometimes described as the capital of the world.\n\"\"\"\nessay_index_summary = \"\"\"\n    Author: Paul Graham. \n    The author grew up painting and writing essays. \n    He wrote a book on Lisp and did freelance Lisp hacking work to support himself. \n    He also became the de facto studio assistant for Idelle Weber, an early photorealist painter. \n    He eventually had the idea to start a company to put art galleries online, but the idea was unsuccessful. \n    He then had the idea to write software to build online stores, which became the basis for his successful company, Viaweb. \n    After Viaweb was acquired by Yahoo!, the author returned to painting and started writing essays online. \n    He wrote a book of essays, Hackers & Painters, and worked on spam filters. \n    He also bought a building in Cambridge to use as an office. \n    He then had the idea to start Y Combinator, an investment firm that would \n    make a larger number of smaller investments and help founders remain as CEO. \n    He and his partner Jessica Livingston ran Y Combinator and funded a batch of startups twice a year. \n    He also continued to write essays, cook for groups of friends, and explore the concept of invented vs discovered in software. \n\n\"\"\"\nindex_summaries = [nyc_index_summary, essay_index_summary]\nnyc_index.index_struct.index_id = \"nyc_index\"\nessay_index.index_struct.index_id = \"essay_index\"\n\n\n# ### Build Keyword Table Index on top of vector indices! \n# \n# We set summaries for each of the NYC and essay indices, and then compose a keyword index on top of it.\n\n# In[ ]:\n\n\n# set query config\nquery_configs = [\n    {\n        \"index_struct_id\": \"nyc_index\",\n        \"index_struct_type\": \"dict\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"similarity_top_k\": 1,\n            \"weaviate_client\": client,\n            \"class_prefix\": \"Nyc_docs\"\n        }\n    },\n    {\n        \"index_struct_id\": \"essay_index\",\n        \"index_struct_type\": \"dict\",\n        \"query_mode\": \"default\",\n        \"query_kwargs\": {\n            \"similarity_top_k\": 1,\n            \"weaviate_client\": client,\n            \"class_prefix\": \"Essay_docs\"\n        }\n    },\n    {\n        \"index_struct_type\": \"keyword_table\",\n        \"query_mode\": \"simple\",\n        \"query_kwargs\": {}\n    },\n]\n\n\n# ### Define Graph\n\n# In[ ]:\n\n\nfrom gpt_index.indices.composability import ComposableGraph\n\n\n# In[ ]:\n\n\ngraph = ComposableGraph.from_indices(\n    GPTSimpleKeywordTableIndex, \n    [nyc_index, essay_index], \n    index_summaries=index_summaries,\n    max_keywords_per_chunk=50)\n\n\n# In[ ]:\n\n\n# [optional] save to disk\ngraph.save_to_disk(\"index_graph.json\")\n\n\n# In[ ]:\n\n\n# [optional] load from disk\ngraph = ComposableGraph.load_from_disk(\"index_graph.json\")\n\n\n# In[ ]:\n\n\n# set Logging to DEBUG for more detailed outputs\n# ask it a question about NYC \nresponse = graph.query(\n    \"What is the weather of New York City like? How cold is it during the winter?\", \n    query_configs=query_configs\n)\n\n\n# In[ ]:\n\n\nprint(str(response))\n\n\n# In[ ]:\n\n\n# Get source of response\nprint(response.get_formatted_sources())\n\n\n# In[ ]:\n\n\n# ask it a question about PG's essay\nresponse = graph.query(\n    \"What did the author do growing up, before his time at Y Combinator?\", \n    query_configs=query_configs\n)\n\n\n# In[ ]:\n\n\nprint(str(response))\n\n\n# In[ ]:\n\n\n# Get source of response\nprint(response.get_formatted_sources())\n\n\n# In[ ]:\n\n\n\n\n"}